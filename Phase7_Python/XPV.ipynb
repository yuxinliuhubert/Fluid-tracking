{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy\n",
    "from scipy import integrate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import norm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import heapq\n",
    "import os\n",
    "from generateTestPositions import generateTestPositions\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import plotly.graph_objects as go\n",
    "matplotlib.use('TkAgg')  # or another interactive backend\n",
    "import random\n",
    "\n",
    "from sympy import symbols, Eq, solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# dataFolder = r\"5\"\n",
    "dataFolder = r\"Synthetic_1particle_linearx\"\n",
    "folderName = r\"C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Phase7_python\\Unsorted\" \n",
    "file_prefix = \"NOS\"\n",
    "\n",
    "# Testing with dummy data...only required when data is not available for code testing...generates projections based on magnification, rotation and known velocity profile\n",
    "\n",
    "NumOfDataPoints = 1\n",
    "clusterness = 0.1 # smaller number the more clustered\n",
    "\n",
    "\n",
    "# Input conditions\n",
    "noise = 1e-3\n",
    "theta_degrees = 1\n",
    "# rev = 2  # revolutions of camera for the entire process\n",
    "# NOS = int(rev * 360 / theta_degrees)\n",
    "NOS = 400\n",
    "NOS_per_section =8  # must be larger than 5 to satisfy equations\n",
    "delta_T = 0.1\n",
    "# delta_T = 1/68\n",
    "camera_speed = 0.5  # in Hz or revolution per second\n",
    "SOD = 38  # mm, Source-Reference Distance\n",
    "ODD = 462  # mm, Reference-Detector (screen) Distance\n",
    "# SOD = 15  # mm, Source-Reference Distance\n",
    "# ODD = 400  # mm, Reference-Detector (screen) Distance\n",
    "\n",
    "\n",
    "offset = [243.5, 97.5]\n",
    "# offset = [0.0,0.0]\n",
    "# pixelResolution = 0.172  # every pixel is equal to mm\n",
    "pixelResolution = 1  # every pixel is equal to mm\n",
    "method = 'acceleration'\n",
    "dataPiling = 'serial'\n",
    "\n",
    "# Auto-calculations of the rest of the parameters derived from the setting above\n",
    "# delta_T = camera_speed * theta_degrees / 360\n",
    "camera_speed = 360* delta_T / theta_degrees\n",
    "shots_per_second = 1 / delta_T\n",
    "\n",
    "# Define the velocity function\n",
    "# v = lambda t: [0.9 * np.sin(t), 0.9 * np.cos(t), 1]\n",
    "\n",
    "# AI conditions\n",
    "learning_rate_2D =1\n",
    "motion_randomness = 3\n",
    "learning_rate_3D =0.3\n",
    "\n",
    "\n",
    "# Pack conditions into a list\n",
    "conditions = [noise, delta_T, NOS, theta_degrees, NOS_per_section, SOD, ODD,method,dataPiling]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate test positions\n",
    "# NumberOfTestPoints = 1\n",
    "# initial_positions = np.zeros((NumOfDataPoints,3))\n",
    "# initial_positions[0] = [6,4,8]\n",
    "# v = []\n",
    "\n",
    "# # v.append(lambda t: [3*np.sin(t), 2*np.cos(t), np.sin(t)])\n",
    "# v.append(lambda t: [0.1,0.05,-0.1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "real_positions_folder = r\"C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Phase7_python\\Real_positions\"\n",
    "\n",
    "print(\"list of files: \",os.listdir(real_positions_folder))\n",
    "sorted_filenames = sorted(os.listdir(real_positions_folder), key=lambda x: int(x.split(file_prefix)[1].split('.csv')[0]))\n",
    "print(len(sorted_filenames))\n",
    "k = 0\n",
    "\n",
    "for fileIndex in range(min(len(sorted_filenames), NOS)):\n",
    "    file = sorted_filenames[fileIndex]\n",
    "    if file.endswith(\".csv\"):\n",
    "        filename = os.path.join(real_positions_folder, file)\n",
    "        input_data = pd.read_csv(filename, header=None)\n",
    "        input_data = np.array(np.transpose(input_data))\n",
    "        if fileIndex == 0:\n",
    "            real_positions = input_data\n",
    "        else:\n",
    "            real_positions = np.row_stack((real_positions, input_data))\n",
    "    # print(\"fileIndex: \",fileIndex)\n",
    "# print(\"real_positions: \",real_positions)\n",
    "# _, columnNum = real_positions.shape\n",
    "# # Create the figure and axes\n",
    "# fig3 = plt.figure()\n",
    "# ax3 = fig3.add_subplot(111, projection='3d')\n",
    "# for i in range(int(columnNum/3)):\n",
    "#     plotting_single(real_positions[:,i*3:i*3+3],i,ax3)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "sorted_positions_folder = r\"C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Phase7_python\\Sorted\"\n",
    "\n",
    "print(\"list of files: \",os.listdir(sorted_positions_folder))\n",
    "sorted_filenames = sorted(os.listdir(sorted_positions_folder), key=lambda x: int(x.split(file_prefix)[1].split('.csv')[0]))\n",
    "print(len(sorted_filenames))\n",
    "k = 0\n",
    "\n",
    "for fileIndex in range(min(len(sorted_filenames), NOS)):\n",
    "    file = sorted_filenames[fileIndex]\n",
    "    if file.endswith(\".csv\"):\n",
    "        filename = os.path.join(sorted_positions_folder, file)\n",
    "        input_data = pd.read_csv(filename, header=None)\n",
    "        input_data = np.array(np.transpose(input_data))\n",
    "        # print(\"input_data: \",input_data)\n",
    "        if fileIndex == 0:\n",
    "            xz_proj = input_data\n",
    "        else:\n",
    "            xz_proj = np.row_stack((xz_proj, input_data))\n",
    "\n",
    "NumOfDataPoints = int(len(xz_proj[0])/2)\n",
    "\n",
    "# calculate synthetic data \n",
    "# xz_proj = np.zeros((NOS, NumOfDataPoints*2))\n",
    "# real_positions = np.zeros((NOS, NumOfDataPoints*3))\n",
    "# # Generate test positions\n",
    "# for i in range(NumOfDataPoints):\n",
    "#     # vel = v[i]\n",
    "#     # xz_proj[:,i*2:i*2+2], real_positions[:,i*3:i*3+3]= generateTestPositions(vel, initial_positions[i], conditions)\n",
    "#     xz_proj[:,i*2:i*2+2], _= generateTestPositions(vel, initial_positions[i], conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# shared functions\n",
    "def map_range(x, x_min, x_max, y_min, y_max):\n",
    "    # Apply linear mapping\n",
    "    scaled = y_min + (y_max - y_min) * ((x - x_min) / (x_max - x_min))\n",
    "\n",
    "    # Apply floor and ceiling conditions\n",
    "    scaled = np.where(x <= x_min, y_min, scaled)\n",
    "    scaled = np.where(x >= x_max, y_max, scaled)\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def rotation(r1, alpha):#why -alpha?\n",
    "\n",
    "\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(-alpha), -np.sin(-alpha), 0],\n",
    "        [np.sin(-alpha),  np.cos(-alpha), 0],\n",
    "        [0,               0,              1]\n",
    "    ])\n",
    "    r2 = np.matmul(rotation_matrix, r1)#does order matter?\n",
    "    return r2\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "def is_package_installed(package):\n",
    "    try:\n",
    "        pkg_resources.get_distribution(package)\n",
    "        return True\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 2D sorting functions\n",
    "class pf:\n",
    "    def __init__(self, alpha,conditions, reconstruction_conditions) -> None:\n",
    "        self.alpha = alpha\n",
    "        self.current_snapShotIndex = 0\n",
    "        self.particle_id = 0\n",
    "        self.particleData_2D = {}\n",
    "        self.shotData = {}\n",
    "        self.original_shotData = {}\n",
    "        self.learning_rate_2D = conditions[0]\n",
    "        self.corrected_learning_rates_data = {}\n",
    "        self.motion_randomness = conditions[1]\n",
    "        self.learning_rate_3D = conditions[2]\n",
    "        self.reconstruction_conditions = reconstruction_conditions.copy()\n",
    "        self.NOS = reconstruction_conditions[2]\n",
    "        self.NOS_per_section = reconstruction_conditions[4]\n",
    "        self.reconstruction_conditions[2] = self.NOS_per_section + 1\n",
    "        self.learning_rate_corrected = False\n",
    "        self.SOD = reconstruction_conditions[5]\n",
    "        self.RDD = reconstruction_conditions[6]\n",
    "\n",
    "        # # Pack conditions into a list\n",
    "        # conditions = [noise, delta_T, NOS, theta_degrees, NOS_per_section, SRD, RDD,method,dataPiling]\n",
    "     # input format, list of tuple of two elements (x,y)\n",
    "\n",
    "    def assign_particle_id(self):\n",
    "        returnID = self.particle_id\n",
    "        self.particle_id += 1\n",
    "        return returnID\n",
    "    \n",
    "    def append(self, snapshot):\n",
    "        # snapshot = snapshot.tolist()\n",
    "        # store the snapshot in a time sequence dictionary\n",
    "        # print(\"current_snapShotIndex: \",self.current_snapShotIndex)\n",
    "        # print(\"current shot: \",snapshot)\n",
    "        # if self.current_snapShotIndex != 0:\n",
    "        #     print(\"previous shot: \", self.shotData[self.current_snapShotIndex - 1])\n",
    "\n",
    "    \n",
    "        if self.current_snapShotIndex not in self.shotData:\n",
    "            self.shotData[self.current_snapShotIndex] = snapshot\n",
    "\n",
    "        if self.current_snapShotIndex not in self.original_shotData:\n",
    "            self.original_shotData[self.current_snapShotIndex] = snapshot.copy()\n",
    "        \n",
    "        \n",
    "        if self.current_snapShotIndex == 0:\n",
    "            self.save_initial_particles(snapshot)\n",
    "        else:\n",
    "            # match previous particles to current\n",
    "\n",
    "            self.match_previous_particle_to_current(snapshot)\n",
    "\n",
    "        # print(\"particleData: \",self.particleData)\n",
    "\n",
    "        self.current_snapShotIndex += 1\n",
    "\n",
    "    def correct_learning_rate(self, learning_rates_data: list):\n",
    "        for i in range(len(learning_rates_data)):\n",
    "            self.corrected_learning_rates_data[i] = learning_rates_data[i]\n",
    "        self.learning_rate_corrected = True\n",
    " \n",
    "    def find_array_in_list(self,target, list_of_arrays):\n",
    "        for idx, arr in enumerate(list_of_arrays):\n",
    "            # print(\"equal between: \",target, \" and \", arr, \" is: \",np.array_equal(target, arr))\n",
    "            if np.array_equal(target, arr):\n",
    "\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_relative_snapshotIndex(self, particle_id, snapshot_index):\n",
    "        if particle_id in self.particleData_2D:\n",
    "            data = self.particleData_2D[particle_id]\n",
    "\n",
    "            # print(\"data with particle id, \",particle_id, \" is: \",data)\n",
    "            # print(\"snapshot_index: \",snapshot_index)\n",
    "            # if self.current_snapShotIndex > 141:\n",
    "            #     print(self.particleData_2D)\n",
    "            \n",
    "            # Check if the given snapshot index exists in the list of snapshot indices\n",
    "            if snapshot_index in data['snapshotIndexList']:\n",
    "                # Find the index of the given snapshot index in the list\n",
    "                index = data['snapshotIndexList'].index(snapshot_index)\n",
    "                \n",
    "\n",
    "                return index\n",
    "\n",
    "        # Return None if the particle or snapshot index is not found\n",
    "        KeyError(\"particle_id or snapshot_index not found\")\n",
    "        return None\n",
    "\n",
    "    def find_closest_particle(self, particle, shot, closest_rank=1):\n",
    "\n",
    "        distances = np.linalg.norm(shot - particle, axis=1)\n",
    "\n",
    "        minDistanceIndex = np.argmin(distances)\n",
    "\n",
    "        if closest_rank > 1:\n",
    "            for i in range(closest_rank - 1):\n",
    "                minDistanceIndex = np.argmin(np.delete(distances, minDistanceIndex))\n",
    "        return minDistanceIndex\n",
    "\n",
    "    def get_default_learning_rate(self):\n",
    "        return self.learning_rate_2D\n",
    "\n",
    "    def get_coordinates_by_snapshot(self, particle_id, snapshot_index):\n",
    "        if particle_id in self.particleData_2D:\n",
    "            data = self.particleData_2D[particle_id]\n",
    "            \n",
    "            # Check if the given snapshot index exists in the list of snapshot indices\n",
    "            if snapshot_index in data['snapshotIndexList']:\n",
    "                # Find the index of the given snapshot index in the list\n",
    "                index = data['snapshotIndexList'].index(snapshot_index)\n",
    "                \n",
    "                # Use the index to access the coordinates\n",
    "                coordinates = data['coords'][index]\n",
    "                return coordinates\n",
    "\n",
    "        # Return None if the particle or snapshot index is not found\n",
    "        return None\n",
    "\n",
    "    def get_original_shotData(self):\n",
    "        return self.original_shotData\n",
    "\n",
    "    def get_particle_id(self, particle,snapshotID, closest_rank=1,tolerance=0.01):\n",
    "        target_snapshot = self.shotData[snapshotID]\n",
    "        closest_particle_id_in_shot = self.find_closest_particle(particle, np.array(self.shotData[snapshotID]),closest_rank)\n",
    "\n",
    "        # print(\"particle in get particle id: \",particle)\n",
    "        # print(\"closest_particle_coor: \",closest_particle_id_in_shot)\n",
    "        # print(\"target_snapshot: \",target_snapshot)\n",
    "        # print(\"particleData with id: \",self.particleData[closest_particle_id_in_shot])\n",
    "        # print(\"snapshotID relative: \",particle_relative_shotID)\n",
    "        # print(\"particleData on this shot: \",self.particleData[closest_particle_id_in_shot]['coords'][particle_relative_shotID])\n",
    "        for particle_id in self.particleData_2D:\n",
    "\n",
    "            # print(\"iterating at particle_id: \",particle_id)\n",
    "            # print(\"particleData_individual: \", self.particleData[particle_id]['coords'][snapshotID])\n",
    "            # print(\"target_snapshot[closest_particle_id_in_shot]: \",target_snapshot[closest_particle_id_in_shot])\n",
    "            # print(\"self.particleData[particle_id]['coords'][snapshotID]: \",self.particleData[particle_id]['coords'])\n",
    "\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            distance = np.linalg.norm(target_snapshot[closest_particle_id_in_shot] - self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "            # print(\"particle_relative_shotID: \",particle_relative_shotID)    \n",
    "            if distance < tolerance:\n",
    "                # print(\"found the particle id: \",particle_id)\n",
    "                # particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "                # print(\"particle_relative_shotID: \",particle_relative_shotID)\n",
    "                print(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "            \n",
    "                return particle_id, self.particleData_2D[particle_id]['coords'][particle_relative_shotID]\n",
    "            \n",
    "\n",
    "                \n",
    "        print(\"not found\")\n",
    "        print(\"orignal particle: \", target_snapshot[closest_particle_id_in_shot], \" minus: \", self.particleData_2D['coords'][-1])\n",
    "        KeyError(\"particle_id not found\")\n",
    "\n",
    "    def get_particle_id_from_available_ids(self,particle, snapshotID,id_list):\n",
    "        target_coordinates = []\n",
    "        \n",
    "        for particle_id in id_list:\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            target_coordinates.append(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "        closest_particle_id_in_shot = id_list[self.find_closest_particle(particle, np.array(target_coordinates))]\n",
    "        return closest_particle_id_in_shot, self.particleData_2D[closest_particle_id_in_shot]['coords'][particle_relative_shotID]\n",
    "\n",
    "    def get_particle_id_from_unmatched_ids(self,particle, snapshotID,matched_id_list):\n",
    "        id_list = list(range(0, len(self.particleData_2D)))\n",
    "        for id in matched_id_list:\n",
    "            id_list.remove(id)\n",
    "        target_coordinates = []\n",
    "        for particle_id in id_list:\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            # print(\"particle_relative_shotID: \",particle_relative_shotID)\n",
    "            target_coordinates.append(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "        # print(\"target_coordinates: \",target_coordinates)\n",
    "        closest_particle_id_in_shot = id_list[self.find_closest_particle(particle, np.array(target_coordinates))]\n",
    "        \n",
    "        return closest_particle_id_in_shot, self.particleData_2D[closest_particle_id_in_shot]['coords'][particle_relative_shotID]\n",
    "\n",
    "    def get_total_num_of_particles(self):\n",
    "        return len(self.particleData_2D)\n",
    "\n",
    "    def get_particle_data(self):\n",
    "        return self.particleData_2D\n",
    "\n",
    "    def historicalLinearVelocity(self, previous_particle_id, num_of_snapshots_to_check, discount_factor=1, direction=\"backward\"):\n",
    "        last10Coordiantes = []\n",
    "\n",
    "        for i in range (1,num_of_snapshots_to_check + 1):\n",
    "            last10Coordiantes.append(self.particleData_2D[previous_particle_id]['coords'][self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - i)])\n",
    "                        # k = k**2\n",
    "                        \n",
    "        velocity_array = np.diff(np.array(last10Coordiantes), axis=0)\n",
    "        final_velocity = np.zeros(2)\n",
    "        k = 0\n",
    "\n",
    "        for i in range(len(velocity_array)):\n",
    "            if direction == \"backward\":\n",
    "                final_velocity += velocity_array[i] * discount_factor**(i+1)\n",
    "            elif direction == \"forward\":\n",
    "                final_velocity += velocity_array[len(velocity_array)-1-i] * discount_factor**(i+1)\n",
    "            else:\n",
    "                # assume backward\n",
    "                final_velocity += velocity_array[i] * discount_factor**(i+1)\n",
    "            k += discount_factor**(i+1)\n",
    "        return final_velocity/k\n",
    "\n",
    "    def match_previous_particle_to_current(self, current_shot):\n",
    "\n",
    "        def is_motion_random(historical_vel, observed_vel, motion_randomness):\n",
    "            threshold = 1e-6\n",
    "            is_random = False\n",
    "            if np.isscalar(historical_vel):\n",
    "                length_of_historical_vel = 1\n",
    "            else:\n",
    "                length_of_historical_vel = len(historical_vel)\n",
    "    \n",
    "            for i in range(length_of_historical_vel):\n",
    "                if np.isscalar(observed_vel):\n",
    "                    compare_observe_vel = observed_vel\n",
    "                else:\n",
    "                    compare_observe_vel = observed_vel[i]\n",
    "\n",
    "                if np.isscalar(historical_vel):\n",
    "                    compare_historical_vel = historical_vel\n",
    "                else:\n",
    "                    compare_historical_vel = historical_vel[i]\n",
    "\n",
    "                if compare_historical_vel <= threshold:\n",
    "                    # stationary, no division\n",
    "                    x = abs(compare_historical_vel - compare_observe_vel) > motion_randomness\n",
    "                else:\n",
    "                    x = abs(compare_historical_vel - compare_observe_vel) > motion_randomness\n",
    "                \n",
    "                if x:\n",
    "                    is_random = True\n",
    "                    break\n",
    "\n",
    "            return is_random\n",
    "\n",
    "        previous_shot = self.shotData[self.current_snapShotIndex - 1]\n",
    "\n",
    "        # create defensive copies of the previous and current shots so we can delete items to keep track without affecting the original data\n",
    "        previous_shot_remain = previous_shot.copy()\n",
    "        \n",
    "        current_shot_remain = current_shot.copy()\n",
    "\n",
    "        \n",
    "\n",
    "        # get the ranked list of particles\n",
    "        ranked_particle_list = self.rank_particle_distances(previous_shot_remain, current_shot_remain, search_radius=10)\n",
    "\n",
    "        matched_particles_id = []\n",
    "\n",
    "\n",
    "        # while there are still particles unmatched, we keep matching\n",
    "        while len(previous_shot_remain) > 0 and len(current_shot_remain) > 0:\n",
    "\n",
    "            # get the closest particle\n",
    "            closest_particles = heapq.heappop(ranked_particle_list)\n",
    "            print(\"closest_particles: \",closest_particles)\n",
    "            \n",
    "            previous_index = closest_particles[3]\n",
    "            current_index = closest_particles[2]\n",
    "            # previous_index = closest_particles[2]\n",
    "            # current_index = closest_particles[1]\n",
    "            current_particle_to_match = current_shot[current_index]\n",
    "            print(\" \")\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"current_snapshot: \",self.current_snapShotIndex)\n",
    "            # [previous_particle_id, prev_particle_coor] = self.get_particle_id(previous_shot[previous_index], self.current_snapShotIndex - 1)\n",
    "            # print(\"particleData: \",self.particleData)\n",
    "            if matched_particles_id is None:\n",
    "                [previous_particle_id, prev_particle_coor] = self.get_particle_id(previous_shot[previous_index], self.current_snapShotIndex - 1)\n",
    "            else:\n",
    "                [previous_particle_id, prev_particle_coor] = self.get_particle_id_from_unmatched_ids(previous_shot[previous_index], self.current_snapShotIndex - 1,matched_particles_id)\n",
    "            # delete the points that are matched from the defensive copies\n",
    "            if not self.find_array_in_list(prev_particle_coor, previous_shot_remain) or not self.find_array_in_list(current_particle_to_match, current_shot_remain):\n",
    "                # print(\"skip this occurance \\n \\n\")\n",
    "                continue\n",
    "            \n",
    "            # delete the points that are matched from the defensive copies\n",
    "            for idx, particle in enumerate(previous_shot_remain):\n",
    "                if np.array_equal(particle, prev_particle_coor):\n",
    "                    del previous_shot_remain[idx]\n",
    "                    break\n",
    "          \n",
    "            # explicit loop to remove the element from the current shot list\n",
    "            for idx, particle in enumerate(current_shot_remain):\n",
    "                if np.array_equal(particle, current_particle_to_match):\n",
    "                    del current_shot_remain[idx]\n",
    "                    break\n",
    "            \n",
    "            print(\"we are matching: \",prev_particle_coor, \" with \", current_particle_to_match, \" particle id: \",previous_particle_id)\n",
    "\n",
    "\n",
    "            if previous_particle_id not in self.particleData_2D:\n",
    "                self.particleData_2D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set(), 'learning_rate': []}\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            if self.current_snapShotIndex > 10:\n",
    "                \n",
    "\n",
    "                estimated_vel_from_historical_velocity = self.historicalLinearVelocity(previous_particle_id, 10, discount_factor=0.9)\n",
    "                \n",
    "                observed_vel = current_shot[current_index] - previous_shot[previous_index]\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                if is_motion_random(estimated_vel_from_historical_velocity, observed_vel, self.motion_randomness):\n",
    "\n",
    "                    print(\"motion randomness detected in particle: \",previous_particle_id)\n",
    "                    # motion randomness is too high, we then conduct a reconstruction to verify if the point is valid\n",
    "                    # 3D reconstruction and re sorting strategy \n",
    "                    previous_relative_index = self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - 1)\n",
    "\n",
    "                    compensation_3D = False\n",
    "                    # make sure we have enough data to do the reconstruction\n",
    "                    if previous_relative_index - self.NOS_per_section > 8 and compensation_3D == True:\n",
    "\n",
    "                        # take the previous NOS_per_section data to do the reconstruction for comparison\n",
    "\n",
    "                        # print()\n",
    "\n",
    "                        previous_2D_shots_selected = self.particleData_2D[previous_particle_id]['coords'][previous_relative_index - self.NOS_per_section : previous_relative_index]\n",
    "                        print(\"length of previous_2D_shots_selected: \",len(previous_2D_shots_selected))\n",
    "\n",
    "                        # \n",
    "                        self.reconstruction_conditions[2] = self.NOS\n",
    "                        previous_estimated_positions_single = Phase4_trace_3d(self.reconstruction_conditions, np.array(self.particleData_2D[previous_particle_id]['coords'][:]))\n",
    "\n",
    "                        self.reconstruction_conditions[2] = self.NOS_per_section + 1\n",
    "                        # take the previous NOS_per_section data and the current shot to be added to do the reconstruction and compare\n",
    "                        current_estimated_positions_single = Phase4_trace_3d(self.reconstruction_conditions, np.row_stack([np.array(self.particleData_2D[previous_particle_id]['coords'][previous_relative_index - self.NOS_per_section + 1 : previous_relative_index + 1]), current_particle_to_match]))\n",
    "\n",
    "                        learning_rate_3D = self.learning_rate_3D\n",
    "                        exploitation_rate_3D = 1 - learning_rate_3D\n",
    "\n",
    "                        # get observation velocity from the reconstruction\n",
    "                        observed_vel_3D = current_estimated_positions_single[-1,:] - current_estimated_positions_single[-2,:]\n",
    "\n",
    "                        # take the last 10 data points to do the historical velocity estimation\n",
    "                        # take the last 10 rows and all columns\n",
    "                        historical_vel_3D = np.mean(np.diff(previous_estimated_positions_single[-10:,:], axis=0))\n",
    "        \n",
    "\n",
    "                        # re-calculate the final position if adjustment is needed\n",
    "                        adjusted_vel_3D = observed_vel_3D\n",
    "                        j = 0\n",
    "                        for vel in adjusted_vel_3D:\n",
    "                            if is_motion_random(historical_vel_3D, observed_vel_3D[j], self.motion_randomness):\n",
    "                                adjusted_vel_3D[j] = historical_vel_3D * exploitation_rate_3D + vel * learning_rate_3D\n",
    "\n",
    "                            j += 1\n",
    "\n",
    "                        adjusted_position_3D = current_estimated_positions_single[-1] + adjusted_vel_3D\n",
    "                        adjusted_position_2D = self.particle_projection(self.alpha, adjusted_position_3D)\n",
    "\n",
    "                        # # now update the estimated position\n",
    "                        # if previous_particle_id not in self.particleData_3D:\n",
    "                        #     self.particleData_3D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set()}\n",
    "                        #     self.particleData_3D[previous_particle_id]['coords'].append(current_estimated_positions_single[:-1])\n",
    "                        #     self.particleData_3D[previous_particle_id]['snapshotIndexList'] = self.particleData_2D[previous_particle_id]['snapshotIndexList']\n",
    "                        #     self.particleData_3D[previous_particle_id]['snapshotIndexSet'] = self.particleData_2D[previous_particle_id]['snapshotIndexSet']\n",
    "                            \n",
    "                            \n",
    "                        # self.particleData_3D[previous_particle_id]['coords'].append(current_estimated_positions_single[-1] + adjusted_vel_3D)\n",
    "\n",
    "                        # # add the new positions with the old, and then take average\n",
    "                        # new_positions[:len(last_positions), :] = (new_positions[:len(last_positions), :] + last_positions) / 2\n",
    "                        # positions_predicted[proj_used_index:proj_used_index + new_positions.shape[0], :] = new_positions\n",
    "                        print(\"adjusted_position_2D: \",adjusted_position_2D)\n",
    "                        final_xy = adjusted_position_2D\n",
    "\n",
    " \n",
    "\n",
    "                    else:\n",
    "                        print(\"not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\")\n",
    "                        # calculate position from learning factor\n",
    "                        if self.learning_rate_corrected:\n",
    "                            learning_rates_2D = self.corrected_learning_rates_data[previous_particle_id]\n",
    "                            print(\"learning_rates_2D: \",learning_rates_2D)\n",
    "                            learning_rate_2D = learning_rates_2D[self.current_snapShotIndex]\n",
    "                        else:\n",
    "                            learning_rate_2D = self.learning_rate_2D\n",
    "                        exploitation_rate_2D = 1 - learning_rate_2D\n",
    "                        final_xy = estimated_vel_from_historical_velocity*exploitation_rate_2D + observed_vel*learning_rate_2D + previous_shot[previous_index]\n",
    "                        print(\"final_xy: \", final_xy)\n",
    "                    \n",
    "                    self.particleData_2D[previous_particle_id]['coords'].append(final_xy)\n",
    "                        # modify shot data to keep the consistency\n",
    "                    self.shotData[self.current_snapShotIndex][current_index] = final_xy\n",
    "                \n",
    "                else:\n",
    "                    self.particleData_2D[previous_particle_id]['coords'].append(current_particle_to_match)\n",
    "                \n",
    "            else:\n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(current_particle_to_match)\n",
    "\n",
    "    \n",
    "            self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "            \n",
    "            self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "\n",
    "            matched_particles_id.append(previous_particle_id)\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        # print(\"Entered compensation mode\")\n",
    "        # if the new snapshot has more particles than the previous one by comparing the length of the remaining particles in the defensive copies\n",
    "        if len(current_shot_remain) > len(previous_shot_remain):\n",
    "            print(\"Entered compensation mode, more current particles than previous\")\n",
    "            # print(\"current_shot_remain: \",current_shot_remain)\n",
    "            # create new unique particles and save them \n",
    "            for particle in current_shot_remain:\n",
    "                previous_particle_id = self.assign_particle_id()\n",
    "                if previous_particle_id not in self.particleData_2D:\n",
    "                    self.particleData_2D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set()}\n",
    "                    \n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(particle)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "\n",
    "        # if the new snapshot has less particles than the previous one\n",
    "        elif len(current_shot_remain) < len(previous_shot_remain):\n",
    "            print(\"Entered compensation mode: more previous particles than current\")\n",
    "            # print(\"previous_shot_remain: \",previous_shot_remain)\n",
    "            # we estimate the unmatched particle with the trajectory of the closest neighbor (current snapshot position - previous snapshot position)\n",
    "            \n",
    "            for prev_particle in previous_shot_remain:\n",
    "\n",
    "                previous_particle_id, prev_particle_coor = self.get_particle_id_from_unmatched_ids(prev_particle_coor, self.current_snapShotIndex-1,matched_particles_id)\n",
    "                \n",
    "                # neighbor strategy\n",
    "                if self.current_snapShotIndex <= 10:\n",
    "                    # print(\"id list: \",matched_particles_id)\n",
    "                    closest_neighbor_particle_id, closest_neighbor_previous_xy = self.get_particle_id_from_available_ids(prev_particle, self.current_snapShotIndex - 1, matched_particles_id)\n",
    "                    # print(\"closest_neighbor_particle_id: \",closest_neighbor_particle_id)\n",
    "                    relativeIndex = self.find_relative_snapshotIndex(closest_neighbor_particle_id, self.current_snapShotIndex)\n",
    "                    # print(relativeIndex)\n",
    "                    closest_neighbor_current_xy = self.particleData_2D[closest_neighbor_particle_id]['coords'][relativeIndex]\n",
    "                    # print(\"closest_neighbor_current_xy: \",closest_neighbor_current_xy)\n",
    "                    # closest_neighbor_previous_xy = self.get_coordinates_by_snapshot(closest_neighbor_particle_id, self.current_snapShotIndex - 1)\n",
    "                    # Calculate the difference between current and previous coordinates (c-p)\n",
    "                    difference_xy = np.array(closest_neighbor_current_xy) - np.array(closest_neighbor_previous_xy)\n",
    "\n",
    "                    estiamted_xy = tuple(np.array(prev_particle_coor) + difference_xy)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    previous_xy = self.particleData_2D[previous_particle_id]['coords'][self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - 1)]\n",
    "                    # historical velocity strategy\n",
    "                    estiamted_xy = self.historicalLinearVelocity(previous_particle_id,10,0.8) + previous_xy\n",
    "\n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(estiamted_xy)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "                self.shotData[self.current_snapShotIndex].append(np.array(estiamted_xy))\n",
    "\n",
    "    def particle_projection(self, alpha, r_0):\n",
    "        r_0_rotated=rotation(r_0,alpha)\n",
    "        _, _, _, _, _, SOD, ODD,_,_ = self.reconstruction_conditions\n",
    "        M_p = (SOD + ODD) / (SOD + r_0_rotated[1])\n",
    "        \n",
    "    \n",
    "        return np.array([M_p * r_0_rotated[0], M_p * r_0_rotated[2]])\n",
    "\n",
    "    def save_initial_particles(self, snapshot):\n",
    "        for particle in snapshot:\n",
    "            particle_id = self.assign_particle_id()\n",
    "            # if self.paricleData is None:\n",
    "            #     self.paricleData = {particle_id: particle}\n",
    "            if particle_id not in self.particleData_2D:\n",
    "                self.particleData_2D[particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set(), 'learning_rate':[]}\n",
    "                \n",
    "            self.particleData_2D[particle_id]['coords'].append(particle)\n",
    "            self.particleData_2D[particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "            self.particleData_2D[particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "            self.particleData_2D[particle_id]['learning_rate'].append(learning_rate_2D)\n",
    "\n",
    "    def rank_particle_distances(self, previous_snapshot, current_snapshot, search_radius):\n",
    "        # new method, ranked by the line distances\n",
    "        # established a heap queue\n",
    "        ranked_particle_heapq = []\n",
    "\n",
    "        for i in range(len(previous_snapshot)):\n",
    "            for j in range(len(current_snapshot)):\n",
    "                equation_end_point = previous_snapshot[i]\n",
    "                current_point = current_snapshot[j]\n",
    "\n",
    "                # get the line of possible existance\n",
    "                line_in_new_basis = self.particle_to_line_in_new_basis(equation_end_point)\n",
    "                \n",
    "                print(\"line_in_new_basis: \",line_in_new_basis)\n",
    "                print(\"current_snapshot[j]: \",current_snapshot[j])\n",
    "                # get the closest particle in the current snapshot to the line\n",
    "                distance = self.distance_to_line(current_point, line_in_new_basis)\n",
    "\n",
    "                # distance = np.linalg.norm(current_point - equation_end_point)\n",
    "\n",
    "                # if the particles are more in the middle, we rely more on close neighbor than matrix transformation\n",
    "                if abs(current_point[0]) < 3 or abs(current_point[1]) < 3:\n",
    "                    learning_factor_stability = 0.9\n",
    "                else:\n",
    "                    learning_factor_stability = 0.6\n",
    "\n",
    "                learning_factor_stability = 0.5/abs(current_point[0]) + 0.5/abs(current_point[1])\n",
    "\n",
    "                # store the particles together by priority of its line distances. j is the index of the curret particle, i is the index of the previous particle\n",
    "                heapq.heappush(ranked_particle_heapq, ((abs(current_point[1] - equation_end_point[1]) + abs(current_point[0] - equation_end_point[0]))* learning_factor_stability + abs(distance *(1-learning_factor_stability)) , abs(current_point[1] - equation_end_point[1]), j, i))\n",
    "            \n",
    "\n",
    "\n",
    "        # #  old method ranked by closest neighbor between particles\n",
    "        # # turn previous snapshot into a N by 2 matrix\n",
    "        # previous_snapshot = np.array(previous_snapshot).reshape(-1,2)\n",
    "\n",
    "        # # grab the x axis\n",
    "        # previous_x = previous_snapshot[:,0]\n",
    "        # # grab the y axis\n",
    "        # previous_y = previous_snapshot[:,1]\n",
    "\n",
    "        # # turn current snapshot into a N by 2 matrix\n",
    "        # current_snapshot = np.array(current_snapshot).reshape(-1,2)\n",
    "\n",
    "        # # grab the x axis\n",
    "        # current_x = current_snapshot[:,0]\n",
    "        # # grab the y axis\n",
    "        # current_y = current_snapshot[:,1]\n",
    "\n",
    "        # # get the large matrix to fix the case where there are different number of particles in the previous and current snapshot\n",
    "        # x_large_matrix = np.tile(previous_x, (len(current_x), 1)) - np.tile(current_x, (len(previous_x), 1)).T\n",
    "        # y_large_matrix = np.tile(previous_y, (len(current_y), 1)) - np.tile(current_y, (len(previous_y), 1)).T\n",
    "\n",
    "        # # print(\"x_large_matrix: \",x_large_matrix)\n",
    "        # # print(\"y_large_matrix: \",y_large_matrix)\n",
    "\n",
    "        # # get the distance matrix\n",
    "        # distance_matrix = np.sqrt(x_large_matrix**2 + y_large_matrix**2)\n",
    "        # # print(\"distance_matrix: \",distance_matrix)\n",
    "\n",
    "        # # create a heap queue to store the ranked particles\n",
    "        # ranked_particle_heapq = []\n",
    "\n",
    "        # # Get the sorted indices of the flattened distance_matrix\n",
    "        # sorted_indices = np.argsort(distance_matrix.ravel())\n",
    "\n",
    "        # # Convert the flattened indices to 2D row and column indices\n",
    "        # row_col_indices = np.unravel_index(sorted_indices, distance_matrix.shape)\n",
    "\n",
    "        # # Print the values in distance_matrix in ascending order along with their row and column indices\n",
    "        # for i in range(len(sorted_indices)):\n",
    "        #     row, col = row_col_indices[0][i], row_col_indices[1][i]\n",
    "        #     # print(f\"Value: {distance_matrix[row, col]}, Row: {row}, Col: {col}\")\n",
    "        #     distance = distance_matrix[row, col]\n",
    "        #     # store the closest particles together. row is the index of the curret particle, col is the index of the previous particle\n",
    "        #     heapq.heappush(ranked_particle_heapq, (distance, 0,row, col))\n",
    "        #     # print(\"original ranked_particle_heapq: \",len(ranked_particle_heapq))\n",
    "\n",
    "        return ranked_particle_heapq\n",
    "\n",
    "    def particle_to_line_in_new_basis(self,projected_particle_coodinates): \n",
    "        # magnification_factor = self.SOD / (self.SOD + self.ODD)\n",
    "        source_3D_coorindates = [0, -self.SOD, 0] # sitting on the y axis in every shot\n",
    "        projected_particle_3D_coodinates = [projected_particle_coodinates[0], self.RDD, projected_particle_coodinates[1]]\n",
    "\n",
    "        # transform the particles to the new rotated basis\n",
    "        transformed_source_3D_coorindates = rotation(source_3D_coorindates, -self.alpha) \n",
    "        transformed_projected_particle_3D_coodinates = rotation(projected_particle_3D_coodinates, -self.alpha)\n",
    "\n",
    "        new_source_3D_coorindates = source_3D_coorindates\n",
    "        # calculate the line equation end particles in the new basis\n",
    "\n",
    "        projected_transformed_source_3D_coorindates = self.find_intersection(new_source_3D_coorindates,transformed_source_3D_coorindates, self.RDD)\n",
    "        projected_transformed_projected_particle_3D_coodinates = self.find_intersection(new_source_3D_coorindates,transformed_projected_particle_3D_coodinates, self.RDD)\n",
    "        # print(\"projected_transformed_source_3D_coorindates: \",projected_transformed_source_3D_coorindates)\n",
    "        # print(\"projected_transformed_projected_particle_3D_coodinates: \",projected_transformed_projected_particle_3D_coodinates)\n",
    "        transformed_source_2D_coorindates = np.array((projected_transformed_source_3D_coorindates[0],projected_transformed_source_3D_coorindates[2]))\n",
    "        transformed_projected_particle_2D_coodinates = np.array((projected_transformed_projected_particle_3D_coodinates[0],projected_transformed_projected_particle_3D_coodinates[2]))\n",
    "\n",
    "\n",
    "\n",
    "        return np.array([transformed_source_2D_coorindates, transformed_projected_particle_2D_coodinates])\n",
    "\n",
    "    # Function to calculate distance from a point to the line\n",
    "    def distance_to_line(self, point, ABMatrix):\n",
    "        line_point1, line_point2 = ABMatrix\n",
    "        line_point1 = np.array([float(elem) for elem in line_point1])\n",
    "        line_point2 = np.array([float(elem) for elem in line_point2])\n",
    "        line_vec = line_point2 - line_point1\n",
    "        point_vec = point - line_point1\n",
    "      \n",
    "        # print(\"line_vec: \",line_vec)\n",
    "        # print(\"Type of line_vec: \",type(line_vec))\n",
    "        # print(\"type of lin vec element: \",type(line_vec[0]))\n",
    "        \n",
    "        line_len = np.linalg.norm(line_vec)\n",
    "        line_unitvec = line_vec / line_len\n",
    "        point_vec_scaled = point_vec / line_len\n",
    "        t = np.dot(line_unitvec, point_vec_scaled)    \n",
    "        nearest = line_point1 + t * line_vec\n",
    "        return np.linalg.norm(nearest - point)\n",
    "    \n",
    "    def find_intersection(self, p1, p2, RDD):\n",
    "        # Convert points to numpy arrays\n",
    "        p1 = np.array(p1)\n",
    "        p2 = np.array(p2)\n",
    "\n",
    "        # Direction vector of the line\n",
    "        direction = p2 - p1\n",
    "\n",
    "        # Check if the line is parallel to the plane\n",
    "        if direction[1] == 0:\n",
    "            return \"No intersection or line lies in the plane\"\n",
    "\n",
    "        # Calculate the t parameter for the line equation\n",
    "        t = (RDD - p1[1]) / direction[1]\n",
    "\n",
    "        # Calculate the intersection point\n",
    "        intersection_point = p1 + t * direction\n",
    "\n",
    "        return intersection_point\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3D reconstruction functions\n",
    "\n",
    "def generateEstimatedPositions(alpha, proj_used_index, N, xz_proj, conditions):\n",
    "    _,delta_T, _, theta_degree, _, SOD, ODD,method,dataPiling = conditions\n",
    "    theta = np.deg2rad(theta_degree)\n",
    "    positions_predicted = np.zeros((N, 3))\n",
    "    \n",
    "    #     # Record the start time\n",
    "    # start_time = time.time() \n",
    "\n",
    "    # use the new method, more time, more accuracy\n",
    "    # values_this_round = proj2r0_acc(xz_proj[proj_used_index : proj_used_index+N-2, :], theta, SRD, RDD, delta_T)\n",
    "    # position_rotated = position_rotated[0]\n",
    "    # velocity_rotated = velocity_rotated[0]\n",
    "    # acc_rotated = acc_rotated[0]\n",
    "\n",
    "    # old method for time efficiency\n",
    "\n",
    "    # print(\"proj_used_index: \",proj_used_index)\n",
    "    # print(proj_used_index+N-2)\n",
    "    # print(\"N: \",N)\n",
    "\n",
    "    # print(\"xz_proj: \",xz_proj)\n",
    "    values_this_round = proj2r0_acc_old(xz_proj[proj_used_index : proj_used_index+N-2, :], theta, SOD, ODD, delta_T)\n",
    "        # Record the end time\n",
    "    # end_time = time.time()\n",
    "\n",
    "    # # Calculate and print the total runtime\n",
    "    # runtime = end_time - start_time\n",
    "    # print(f\"The runtime of projr20 is {runtime} seconds.\")\n",
    "    if method == 'acceleration':\n",
    "        x0, y0, z0, u, v, w, a_x, a_y, a_z = values_this_round\n",
    "        # print(\"values_this_round\",values_this_round)\n",
    "        position_rotated =  np.transpose(rotation([x0, y0, z0], alpha))\n",
    "        # print(\"position_rotated\",position_rotated)\n",
    "        x0, y0, z0 = position_rotated[0][0], position_rotated[0][1], position_rotated[0][2]\n",
    "        positions_predicted[0, :] = position_rotated\n",
    "        velocity_rotated = np.transpose(rotation([u, v, w], alpha))\n",
    "        u, v, w = velocity_rotated[0][0], velocity_rotated[0][1], velocity_rotated[0][2]\n",
    "        acc_rotated =  np.transpose(rotation([a_x, a_y, a_z], alpha))\n",
    "        a_x, a_y, a_z = acc_rotated[0][0], acc_rotated[0][1], acc_rotated[0][2]\n",
    "\n",
    "\n",
    "        for j in range(1, N):\n",
    "            time = delta_T * (j)\n",
    "            positions_predicted[j, :] = [x0+u*time+0.5*a_x*time**2, y0+v*time+0.5*a_y*time**2, z0+w*time+0.5*a_z*time**2]\n",
    "\n",
    "\n",
    "    elif method == 'linear':\n",
    "        x0, y0, z0, u, v, w,_,_,_ = values_this_round\n",
    "        position_rotated = np.transpose(rotation([x0, y0, z0], alpha))\n",
    "        x0, y0, z0 = position_rotated\n",
    "        positions_predicted[0, :] = position_rotated\n",
    "        velocity_rotated = np.transpose(rotation([u, v, w], alpha))\n",
    "        u, v, w = velocity_rotated\n",
    "\n",
    "        for j in range(1, N):\n",
    "            time = delta_T * (j)\n",
    "            positions_predicted[j, :] = [x0+u*time, y0+v*time, z0+w*time]\n",
    "\n",
    "    return positions_predicted   \n",
    "\n",
    "def proj2r0_acc_old(proj, theta, SOD, ODD, delta_T):\n",
    "    NOS = len(proj)\n",
    "    # print(proj)\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = 2 * NOS + 2 * (NOS - 1)\n",
    "    col_number_A = 1 + 2 * NOS\n",
    "    # print(\"row_number_A: \",row_number_A)\n",
    "    # print(\"col_number_A: \",col_number_A)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A,1))\n",
    "    \n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = proj[j]\n",
    "        A[2*j, 0] = 1\n",
    "        A[2*j, 2*j+2] = -zi_j / SDD\n",
    "        b[2*j] = zi_j * SOD / SDD\n",
    "        \n",
    "        A[2*j+1, 2*j+1] = -1\n",
    "        A[2*j+1, 2*j+2] = xi_j / SDD\n",
    "        b[2*j+1] = -xi_j * SOD / SDD\n",
    "    \n",
    "    x = 2 * NOS\n",
    "    for k in range(1, NOS):\n",
    "        A[x:x+2, 2*k+1:2*k+3] = [[-1, 0], [0, -1]]\n",
    "        A[x:x+2, 1:3] = [[np.cos(theta*(k)), np.sin(theta*(k))], [-np.sin(theta*(k)), np.cos(theta*(k))]]\n",
    "        x += 2\n",
    "        \n",
    "    A = np.pad(A, ((0, 0), (0, 6)), 'constant')\n",
    "    new_col_num = A.shape[1]\n",
    "    \n",
    "    for j in range(NOS):\n",
    "        A[2*j, new_col_num-4] = delta_T * (j)\n",
    "        A[2*j, new_col_num-1] = 0.5 * (delta_T * (j))**2\n",
    "        \n",
    "    IoR = 2 * NOS\n",
    "    for k in range(1, NOS):\n",
    "        A[IoR, new_col_num-6] = np.cos(theta * k) * delta_T * k\n",
    "        A[IoR+1, new_col_num-6] = -np.sin(theta * k) * delta_T * k\n",
    "        \n",
    "        A[IoR, new_col_num-5] = np.sin(theta * k) * delta_T * k\n",
    "        A[IoR+1, new_col_num-5] = np.cos(theta * k) * delta_T * k\n",
    "        \n",
    "        A[IoR, new_col_num-3] = 0.5 * np.cos(theta * k) * (delta_T * k)**2\n",
    "        A[IoR+1, new_col_num-3] = -np.sin(theta * k) * 0.5 * (delta_T * k)**2\n",
    "        \n",
    "        A[IoR, new_col_num-2] = 0.5 * np.sin(theta * k) * (delta_T * k)**2\n",
    "        A[IoR+1, new_col_num-2] = 0.5 * np.cos(theta * k) * (delta_T * k)**2\n",
    "        IoR += 2\n",
    "        \n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 =[x[1], x[2], x[0], x[new_col_num-6], x[new_col_num-5], x[new_col_num-4], x[new_col_num-3], x[new_col_num-2], x[new_col_num-1]]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_acc(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    "    A = np.hstack([A, np.zeros((A.shape[0], 6))])\n",
    "    new_col_num = A.shape[1]\n",
    "    u_ind, v_ind, w_ind, ax_ind, ay_ind, az_ind = new_col_num - 6, new_col_num - 5, new_col_num - 4, new_col_num - 3, new_col_num - 2, new_col_num - 1\n",
    "\n",
    "    for j in range(NOS):\n",
    "        A[2 * j, w_ind] = dt * j\n",
    "        A[2 * j, az_ind] = 0.5 * (dt * j)**2\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        T2 = dt * k\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            T1 = T2 - (i+1) * dt\n",
    "            theta_prime = theta * (k - i - 1)\n",
    "            A[IoR, u_ind] = (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, u_ind] = (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, v_ind] = (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, v_ind] = (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, ax_ind] = 0.5 * (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR + 1, ax_ind] = 0.5 * (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR, ay_ind] = 0.5 * (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR + 1, ay_ind] = 0.5 * (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2**2 - T1**2)\n",
    "            IoR += 2\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], x[u_ind], x[v_ind], x[w_ind], x[ax_ind], x[ay_ind], x[az_ind]]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_vel(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    "    A = np.hstack([A, np.zeros((A.shape[0], 3))])\n",
    "    new_col_num = A.shape[1]\n",
    "    u_ind, v_ind, w_ind = new_col_num - 3, new_col_num - 2, new_col_num - 1\n",
    "\n",
    "    for j in range(NOS):\n",
    "        A[2 * j, w_ind] = dt * j\n",
    "        \n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        T2 = dt * k\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            T1 = T2 - (i+1) * dt\n",
    "            theta_prime = theta * (k - i - 1)\n",
    "            A[IoR, u_ind] = (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, u_ind] = (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, v_ind] = (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, v_ind] = (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            IoR += 2\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], x[u_ind], x[v_ind], x[w_ind], 0,0,0]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_stationary(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    " \n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], 0,0,0, 0,0,0]\n",
    "    return r0\n",
    "\n",
    "def Phase4_trace_3d(conditions, xz_proj):\n",
    "    _,delta_T, NOS, theta_degree, N, SOD, ODD,method,dataPiling = conditions\n",
    "\n",
    "    theta = np.deg2rad(theta_degree)\n",
    "\n",
    "    proj_used_index = 0\n",
    "    \n",
    "    NOS = int(conditions[2])  # Convert to int before using\n",
    "    positions_predicted = np.zeros((NOS, 3))\n",
    "    \n",
    "    NOS_per_section = N\n",
    "    prev_NOS_section = NOS_per_section\n",
    "    print(\"NOS: \",NOS)\n",
    "    print(\"NOS_per_Section: \",NOS_per_section) \n",
    "\n",
    "    if dataPiling == 'serial':\n",
    "        while proj_used_index < NOS:\n",
    "            # print(\"project Used indes after: \",proj_used_index)\n",
    "            alpha = -theta*(proj_used_index)\n",
    "            # print(\"I ran it again after\")\n",
    "            temp = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "            positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = temp\n",
    "            # print(\"normal called\")\n",
    "\n",
    "            proj_used_index += NOS_per_section \n",
    "            # proj_used_index += 1\n",
    "\n",
    "            if abs(NOS - proj_used_index) < N:\n",
    "                # NOS_per_section = NOS - proj_used_index + 1\n",
    "                # alpha = -theta*(proj_used_index)\n",
    "                # temp = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section-1,xz_proj,conditions)\n",
    "                # positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = temp\n",
    "                # proj_used_index += NOS_per_section\n",
    "\n",
    "                prev_proj_index = proj_used_index\n",
    "                proj_used_index = NOS - NOS_per_section\n",
    "                alpha = -theta*(proj_used_index)\n",
    "                last_positions = positions_predicted[proj_used_index : prev_proj_index, :]\n",
    "                # print(\"while project used index is less than NOS larger loop: \", xz_proj)\n",
    "                new_positions = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "                # print('prev',prev_proj_index)\n",
    "                # print('proj_index',proj_used_index)\n",
    "                # print('last positions',last_positions)\n",
    "                # print('new positions',new_positions)\n",
    "                combined_positions = np.concatenate([(new_positions[:len(last_positions), :] + last_positions)/2, new_positions[len(last_positions):, :]], axis=0)\n",
    "\n",
    "                positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = combined_positions\n",
    "                # print(\"retro called\")\n",
    "                proj_used_index += NOS_per_section + 1\n",
    "                # print(\"proj_used_index_new: \",proj_used_index)\n",
    "                \n",
    "\n",
    "    elif dataPiling == 'overlap':\n",
    "        for i in range(round(NOS - N)):\n",
    "            print(\"iterations: \",i)\n",
    "            alpha = -theta * (proj_used_index - 1)  # alpha is for tracking the degree rotated from the 1st shot\n",
    "\n",
    "            if proj_used_index == 1:\n",
    "                positions_predicted[proj_used_index:N+1, :] = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "            else:\n",
    "                # take every N shots from every index, and take average of them\n",
    "                last_positions = positions_predicted[proj_used_index: proj_used_index + prev_NOS_section - 1, :]\n",
    "\n",
    "                \n",
    "                new_positions = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "        \n",
    "\n",
    "\n",
    "                new_positions[:len(last_positions), :] = (new_positions[:len(last_positions), :] + last_positions) / 2\n",
    "                positions_predicted[proj_used_index:proj_used_index + new_positions.shape[0], :] = new_positions\n",
    "      \n",
    "\n",
    "            proj_used_index += 1\n",
    "\n",
    "\n",
    "    return positions_predicted\n",
    "\n",
    "def smooth_points(estimated_positions, method, frame_size):\n",
    "    \"\"\"\n",
    "    Smoothens the given 3D estimated positions using one of the specified methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - estimated_positions (ndarray): Nx3 array of estimated 3D positions.\n",
    "    - method (str): Smoothing method ('avg', 'sg', or 'cb').\n",
    "    - frame_size (int): Window size for moving average or Savitzky-Golay filter.\n",
    "    \n",
    "    Returns:\n",
    "    - ndarray: Nx3 array of smoothened 3D positions.\n",
    "    \"\"\"\n",
    "    # Initialize the filtered measurements with the original data\n",
    "    filtered_measurements = estimated_positions.copy()\n",
    "    \n",
    "    # Moving Average\n",
    "    if method == 'avg':\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            filtered_measurements[:, i] = np.convolve(estimated_positions[:, i], np.ones(frame_size)/frame_size, mode='same')\n",
    "    \n",
    "    # Savitzky-Golay Filter\n",
    "    elif method == 'sg':\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            filtered_measurements[:, i] = savgol_filter(estimated_positions[:, i], frame_size, 2)\n",
    "    \n",
    "    # Cubic Smoothing Spline\n",
    "    elif method == 'cb':\n",
    "        x = np.arange(estimated_positions.shape[0])\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            spl = UnivariateSpline(x, estimated_positions[:, i], s=0.5)\n",
    "            filtered_measurements[:, i] = spl(x)\n",
    "    \n",
    "    return filtered_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# graphing functions\n",
    "def number_to_binary_list(number):\n",
    "    binary_str = bin(number)[2:]  # Convert to binary and remove the '0b' prefix\n",
    "    binary_str = binary_str.zfill(3)  # Pad with zeros to make sure it has 3 digits\n",
    "    binary_list = np.array([int(b) for b in binary_str] ) # Convert each binary digit to integer\n",
    "    return binary_list\n",
    "\n",
    "# def plotting_single(positions, particle_id,ax, label='particle'):\n",
    "#     NOS, _ = positions.shape\n",
    "#     particle_id=int(particle_id)+1\n",
    "#     if particle_id<7 and particle_id>=0:\n",
    "#         col=number_to_binary_list(int(particle_id))\n",
    "#     else:\n",
    "#         col=np.array([random.uniform(0.6, 1) for _ in range(3)])\n",
    "#         print(col)\n",
    "\n",
    "    \n",
    "#     # Plot the curve with gradually changing color\n",
    "#     for i in range(NOS - 1):\n",
    "#         color_brightness = (i/(NOS-1))*col\n",
    "        \n",
    "#         x1, y1, z1 = positions[i]\n",
    "#         x2, y2, z2 = positions[i+1]\n",
    "        \n",
    "#         # Draw a line segment with the computed color\n",
    "#         ax.plot([x1, x2], [y1, y2], [z1, z2], color=color_brightness)\n",
    "#     ax.plot([x1, x2], [y1, y2], [z1, z2], color=color_brightness,label=label+str(particle_id-1))\n",
    "#     # Label the axes\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_zlabel('Z')\n",
    "#     plt.legend()\n",
    "\n",
    "def plotting_single(fig, positions, particle_id):\n",
    "    # print(\"positions: \",positions)\n",
    "    NOS, _ = positions.shape\n",
    "    particle_id = int(particle_id) + 1\n",
    "    if particle_id < 7 and particle_id >= 0:\n",
    "        col = number_to_binary_list(int(particle_id))\n",
    "    else:\n",
    "        col = np.array([random.uniform(0.6, 1) for _ in range(3)])\n",
    "\n",
    "    # # Create a Plotly figure\n",
    "    # fig = go.Figure()\n",
    "\n",
    "    # Plot the curve with gradually changing color\n",
    "    # for i in range(NOS - 1):\n",
    "        # color_brightness = (i / (NOS - 1)) * col\n",
    "        # x1, y1, z1 = positions[i]\n",
    "        # x2, y2, z2 = positions[i + 1]\n",
    "\n",
    "\n",
    "        # Add a line segment to the figure\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=positions[:, 0],\n",
    "        y=positions[:, 1],\n",
    "        z=positions[:, 2],\n",
    "        mode='lines',\n",
    "        name='Particle ' + str(particle_id - 1),\n",
    "        # showlegend=True  # Only for traces you want in the legend\n",
    "        # line=dict(color='rgb({},{},{})'.format(*color_brightness*255),\n",
    "        #           width=10)\n",
    "    ))\n",
    "\n",
    "    # Update layout for axes labels and legend\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        legend_title_text='Particle ' + str(particle_id - 1)\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# def show_in_window(fig):\n",
    "#     import sys, os\n",
    "#     import plotly.offline\n",
    "#     from PyQt5.QtCore import QUrl\n",
    "#     from PyQt5.QtWebEngineWidgets import QWebEngineView\n",
    "#     from PyQt5.QtWidgets import QApplication\n",
    "    \n",
    "#     plotly.offline.plot(fig, filename='name.html', auto_open=False)\n",
    "    \n",
    "#     app = QApplication(sys.argv)\n",
    "#     web = QWebEngineView()\n",
    "#     file_path = os.path.abspath(os.path.join(os.path.dirname(\"Phase7_Python\"), \"name.html\"))\n",
    "#     web.load(QUrl.fromLocalFile(file_path))\n",
    "#     web.show()\n",
    "#     sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# file processing function\n",
    "def rename_files_replace_space(directory_path):\n",
    "    # Get a list of all files in the directory\n",
    "    filenames = os.listdir(directory_path)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Replace spaces with underscores\n",
    "        new_filename = filename.replace('_', '')\n",
    "        \n",
    "        # Construct the full old and new file paths\n",
    "        old_filepath = os.path.join(directory_path, filename)\n",
    "        new_filepath = os.path.join(directory_path, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# main function starts here\n",
    "# define different learning rates for investigating different sorting and smoothing model\n",
    "\n",
    "alpha = np.radians(theta_degrees)  # Example rotation angle in radians\n",
    "\n",
    "rates_conditions = [learning_rate_2D, motion_randomness, learning_rate_3D, NOS_per_section]\n",
    "path_finder = pf(alpha,rates_conditions,conditions)\n",
    "\n",
    "print(\"list of files: \",os.listdir(folderName))\n",
    "sorted_filenames = sorted(os.listdir(folderName), key=lambda x: int(x.split(file_prefix)[1].split('.csv')[0]))\n",
    "# print(len(sorted_filenames[0]))\n",
    "k = 0\n",
    "for fileIndex in range(min(len(sorted_filenames), NOS)):\n",
    "    file = sorted_filenames[fileIndex]\n",
    "    print(\"file: \",file)\n",
    "    if file.endswith(\".csv\"):\n",
    "        filename = os.path.join(folderName, file)\n",
    "        input_data = pd.read_csv(filename, header=None)\n",
    "        input_data = np.array(np.transpose(input_data))\n",
    "        values =  input_data[0]\n",
    "        print(\"values:\", values)\n",
    "        print(\"read file: \", filename)\n",
    "\n",
    "        paired_values = []\n",
    "        i = 0\n",
    "        for j in range(len(values)//2):\n",
    "            \n",
    "            inputList = (values[i:i+2]- offset)*pixelResolution\n",
    "           \n",
    "            # input format, list of tuple of two elements (x,y)\n",
    "            paired_values.append(inputList)\n",
    "\n",
    "            # scambled_values = random.shuffle(paired_values.copy())\n",
    "            # print(\"paired values:\", paired_values)\n",
    "            \n",
    "            i += 2\n",
    "\n",
    "        # print(\"paired values:\", paired_values)\n",
    "        # paired_values.append(xz_proj[k])\n",
    "        path_finder.append(paired_values)\n",
    "        k += 1\n",
    "\n",
    "# Assuming path_finder.get_particle_data() returns your data as a dictionary\n",
    "sorted_particle_data = path_finder.get_particle_data()\n",
    "print(\"sorted_particle_data: \",sorted_particle_data)\n",
    "\n",
    "NumOfDataPoints = len(sorted_particle_data)\n",
    "print(\"NumOfDataPoints: \", NumOfDataPoints)\n",
    "\n",
    "print(sorted_particle_data)\n",
    "estimated_positions = np.zeros((NOS,3*NumOfDataPoints))\n",
    "\n",
    "\n",
    "for i in range(NumOfDataPoints):\n",
    "    # print(\"inline, \",np.array(sorted_particle_data[i]['coords']))\n",
    "    estimated_positions_single = Phase4_trace_3d(conditions, np.array(sorted_particle_data[i]['coords']))\n",
    "    # print(\"completed\")\n",
    "    estimated_positions_single = smooth_points(estimated_positions_single, 'sg',NOS_per_section)\n",
    "\n",
    "    estimated_positions[:,i*3:i*3+3] = estimated_positions_single\n",
    "\n",
    "estimated_positions_byPass = np.zeros((NOS,3*NumOfDataPoints))\n",
    "# extra synthetic data points (bypassing sorting)\n",
    "for i in range(NumOfDataPoints):\n",
    "    estimated_positions_single = smooth_points(Phase4_trace_3d(conditions, xz_proj[:,i*2:i*2+2]), 'sg',NOS_per_section)\n",
    "    print(\"count\",i)\n",
    "    estimated_positions_byPass[:,i*3:i*3+3] = estimated_positions_single\n",
    "\n",
    "estimated_positions_new = estimated_positions\n",
    "estimated_positions_graph = np.column_stack((estimated_positions, estimated_positions_byPass, real_positions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# %% recalculate learning rates\n",
    "learning_rate_floor = path_finder.get_default_learning_rate()\n",
    "learning_rate_floor = 1\n",
    "learning_rate_ceiling = 1\n",
    "\n",
    "# empirical, adjust here\n",
    "min_distance_to_origin = 0.6\n",
    "max_distance_to_origin = 6\n",
    "\n",
    "learning_rate_lists = {}\n",
    "\n",
    "for i in range(path_finder.get_total_num_of_particles()):\n",
    "    estimated_positions_individual = estimated_positions_new[:,i*3:i*3+3]\n",
    "\n",
    "    learning_rate_lists[i] = map_range(np.linalg.norm(estimated_positions_individual, axis=1), min_distance_to_origin, max_distance_to_origin, learning_rate_floor, learning_rate_ceiling)\n",
    "\n",
    "print(\"learning_rate_lists: \",learning_rate_lists)\n",
    "\n",
    "corrected_path_finder = pf(alpha,rates_conditions,conditions)\n",
    "\n",
    "corrected_path_finder.correct_learning_rate(learning_rate_lists)\n",
    "\n",
    "shotData = path_finder.get_original_shotData()\n",
    "for i in range(len(shotData)):\n",
    "        # print(\"shotData\",self.shotData[i])\n",
    "        corrected_path_finder.append(shotData[i])\n",
    "# corrected_path_finder.re_run(path_finder.get_original_shotData())\n",
    "\n",
    "sorted_particle_data_corrected = corrected_path_finder.get_particle_data()\n",
    "\n",
    "# Convert the NumPy matrix to a Pandas DataFrame\n",
    "outputMatrix = sorted_particle_data_corrected[0]['coords']\n",
    "for i in range(1,NumOfDataPoints):\n",
    "     outputMatrix = np.column_stack((outputMatrix, sorted_particle_data_corrected[i]['coords']))\n",
    "df = pd.DataFrame(outputMatrix)\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel(\"sorted_particle_data_corrected.xlsx\", index=False)\n",
    "# Convert the NumPy matrix to a Pandas DataFrame\n",
    "df = pd.DataFrame(xz_proj)\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel(\"xz_proj.xlsx\", index=False)\n",
    "\n",
    "estimated_positions_corrected = np.zeros((NOS,3*NumOfDataPoints))\n",
    "for i in range(NumOfDataPoints):\n",
    "    print(\"sorted_particle_data_corrected[i]['coords']\",sorted_particle_data_corrected[i]['coords'])\n",
    "    estimated_positions_single = Phase4_trace_3d(conditions, np.array(sorted_particle_data_corrected[i]['coords']))\n",
    "    estimated_positions_single = smooth_points(estimated_positions_single, 'sg',NOS_per_section)\n",
    "\n",
    "    estimated_positions_corrected[:,i*3:i*3+3] = estimated_positions_single\n",
    "\n",
    "# estimated_positions_corrected_new = estimated_positions_corrected\n",
    "\n",
    "# estimated_positions_corrected_graph = np.column_stack((estimated_positions_corrected,estimated_positions_byPass, real_positions))\n",
    "\n",
    "estimated_positions_corrected_graph = np.column_stack((estimated_positions_corrected,real_positions))\n",
    "# estimated_positions_corrected_graph= estimated_positions_corrected_new\n",
    "\n",
    "print(\"estimated_positions_corrected_graph: \",estimated_positions_corrected_graph)\n",
    "\n",
    "# Convert the NumPy matrix to a Pandas DataFrame\n",
    "df = pd.DataFrame(estimated_positions_corrected_graph)\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "# df.to_excel(\"estimated_positions_corrected_graph.xlsx\", index=False)\n",
    "\n",
    "# NumOfDataPoints += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
<<<<<<< Updated upstream
      "\u001b[1;31m0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "\u001b[1;31m0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "\u001b[1;31m0.00s - to python to disable frozen modules.\n",
      "\u001b[1;31m0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "\u001b[1;31mBad address (C:\\projects\\libzmq\\src\\epoll.cpp:100). \n",
=======
      "\u001b[1;31mThis version of python seems to be incorrectly compiled\n",
      "\u001b[1;31m(internal generated filenames are not absolute).\n",
      "\u001b[1;31mThis may make the debugger miss breakpoints.\n",
      "\u001b[1;31mRelated bug: http://bugs.python.org/issue1666807\n",
      "\u001b[1;31mBad address (C:\\ci\\zeromq_1616055400030\\work\\src\\epoll.cpp:100). \n",
>>>>>>> Stashed changes
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"shape of estimated_positions_graph: \",estimated_positions_graph.shape)\n",
    "import plotly.graph_objs as go\n",
    "# Create the figure and axes\n",
    "# fig = plt.figure()\n",
    "fig = go.Figure()\n",
    "# fig.update_layout(\n",
    "#     scene=dict(\n",
    "#         xaxis_title='X',\n",
    "#         yaxis_title='Y',\n",
    "#         zaxis_title='Z'\n",
    "#     ),\n",
    "#     legend_title_text='Particles'\n",
    "# )\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "_, column = estimated_positions_graph.shape\n",
    "for i in range(column//3):\n",
    "    # plotting_single(estimated_positions_graph[:,i*3:i*3+3],i,ax)\n",
    "    plotting_single(fig, estimated_positions_graph[:,i*3:i*3+3],i)\n",
    "\n",
    "# NumOfDataPoints -=2\n",
    "# print(\"NumOfDataPoints: \",NumOfDataPoints)\n",
    "\n",
    "fig2 = go.Figure()\n",
    "# ax2 = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "x_min = y_min = z_min = float('inf')\n",
    "x_max = y_max = z_max = float('-inf')\n",
    "print(NumOfDataPoints)\n",
    "_, column = estimated_positions_corrected_graph.shape\n",
    "print(\"estiamted corrected shape:\",estimated_positions_corrected_graph.shape)\n",
    "for i in range(column//3):\n",
    "    plotting_single(fig2, estimated_positions_corrected_graph[:,i*3:i*3+3],i)\n",
    "    x_min = min(min(estimated_positions_corrected_graph[:,i*3]), x_min)\n",
    "    x_max = max(max(estimated_positions_corrected_graph[:,i*3]), x_max)\n",
    "    y_min = min(min(estimated_positions_corrected_graph[:,i*3+1]), y_min)\n",
    "    y_max = max(max(estimated_positions_corrected_graph[:,i*3+1]), y_max)\n",
    "    z_min = min(min(estimated_positions_corrected_graph[:,i*3+2]), z_min)\n",
    "    z_max = max(max(estimated_positions_corrected_graph[:,i*3+2]), z_max)\n",
    "\n",
    "    # print(\"estimated_positions_corrected_graph[:,i*3:i*3+3]\",estimated_positions_corrected_graph[:,i*3:i*3+3])\n",
    "# plotting_single(estimated_positions_corrected_graph,i,ax2)\n",
    "\n",
    "print(x_min, x_max, y_min, y_max, z_min, z_max)\n",
    "# if x_max - x_min < 1e-3: \n",
    "#     ax2.set_xlim3d(x_min-1, x_max+1)\n",
    "\n",
    "# if y_max - y_min < 1e-3:\n",
    "#     ax2.set_ylim3d(y_min-1, y_max+1)\n",
    "\n",
    "# if z_max - z_min < 1e-3:\n",
    "#     ax2.set_zlim3d(z_min-1, z_max+1)\n",
    "\n",
    "# show_in_window(fig)\n",
    "# show_in_window(fig2)\n",
    "fig.show()\n",
    "fig2.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
