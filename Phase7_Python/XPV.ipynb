{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter parameters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# scipy\n",
    "from scipy import integrate\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.stats import norm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import heapq\n",
    "import os\n",
    "from generateTestPositions import generateTestPositions\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # or another interactive backend\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderName = r\"C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\"\n",
    "\n",
    "# Testing with dummy data...only required when data is not available for code testing...generates projections based on magnification, rotation and known velocity profile\n",
    "\n",
    "NumOfDataPoints = 1\n",
    "clusterness = 0.1 # smaller number the more clustered\n",
    "\n",
    "\n",
    "# Input conditions\n",
    "noise = 1e-3\n",
    "theta_degrees = 20\n",
    "rev = 2  # revolutions of camera for the entire process\n",
    "NOS = int(rev * 360 / theta_degrees)\n",
    "NOS = 18\n",
    "NOS_per_section =18  # must be larger than 5 to satisfy equations\n",
    "delta_T = 0.015\n",
    "# delta_T = 1/68\n",
    "camera_speed = 0.5  # in Hz or revolution per second\n",
    "SOD = 38  # mm, Source-Reference Distance\n",
    "ODD = 462  # mm, Reference-Detector (screen) Distance\n",
    "# SOD = 15  # mm, Source-Reference Distance\n",
    "# ODD = 400  # mm, Reference-Detector (screen) Distance\n",
    "\n",
    "\n",
    "offset = [243.5, 97.5]\n",
    "# offset = [0.0,0.0]\n",
    "pixelResolution = 0.172  # every pixel is equal to mm\n",
    "method = 'acceleration'\n",
    "dataPiling = 'serial'\n",
    "\n",
    "# Auto-calculations of the rest of the parameters derived from the setting above\n",
    "# delta_T = camera_speed * theta_degrees / 360\n",
    "camera_speed = 360* delta_T / theta_degrees\n",
    "shots_per_second = 1 / delta_T\n",
    "\n",
    "# Define the velocity function\n",
    "# v = lambda t: [0.9 * np.sin(t), 0.9 * np.cos(t), 1]\n",
    "\n",
    "# AI conditions\n",
    "learning_rate_2D =0.3\n",
    "motion_randomness = 3\n",
    "learning_rate_3D =0.3\n",
    "\n",
    "\n",
    "# Pack conditions into a list\n",
    "conditions = [noise, delta_T, NOS, theta_degrees, NOS_per_section, SOD, ODD,method,dataPiling]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test positions\n",
    "NumberOfTestPoints = 1\n",
    "initial_positions = np.zeros((NumOfDataPoints,3))\n",
    "initial_positions[0] = [6,4,8]\n",
    "v = []\n",
    "\n",
    "# v.append(lambda t: [3*np.sin(t), 2*np.cos(t), np.sin(t)])\n",
    "v.append(lambda t: [0.1,0.05,-0.1])\n",
    "\n",
    "# calculate synthetic data \n",
    "xz_proj = np.zeros((NOS, NumOfDataPoints*2))\n",
    "real_positions = np.zeros((NOS, NumOfDataPoints*3))\n",
    "# Generate test positions\n",
    "for i in range(NumOfDataPoints):\n",
    "    vel = v[i]\n",
    "    xz_proj[:,i*2:i*2+2], real_positions[:,i*3:i*3+3]= generateTestPositions(vel, initial_positions[i], conditions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared functions\n",
    "def map_range(x, x_min, x_max, y_min, y_max):\n",
    "    # Apply linear mapping\n",
    "    scaled = y_min + (y_max - y_min) * ((x - x_min) / (x_max - x_min))\n",
    "\n",
    "    # Apply floor and ceiling conditions\n",
    "    scaled = np.where(x <= x_min, y_min, scaled)\n",
    "    scaled = np.where(x >= x_max, y_max, scaled)\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def rotation(r1, alpha):#why -alpha?\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(-alpha), -np.sin(-alpha), 0],\n",
    "        [np.sin(-alpha),  np.cos(-alpha), 0],\n",
    "        [0,               0,              1]\n",
    "    ])\n",
    "    r2 = np.matmul(rotation_matrix, r1)#does order matter?\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D sorting functions\n",
    "class pf:\n",
    "    def __init__(self, alpha,conditions, reconstruction_conditions) -> None:\n",
    "        self.alpha = alpha\n",
    "        self.current_snapShotIndex = 0\n",
    "        self.particle_id = 0\n",
    "        self.particleData_2D = {}\n",
    "        self.shotData = {}\n",
    "        self.original_shotData = {}\n",
    "        self.learning_rate_2D = conditions[0]\n",
    "        self.corrected_learning_rates_data = {}\n",
    "        self.motion_randomness = conditions[1]\n",
    "        self.learning_rate_3D = conditions[2]\n",
    "        self.reconstruction_conditions = reconstruction_conditions.copy()\n",
    "        self.NOS = reconstruction_conditions[2]\n",
    "        self.NOS_per_section = reconstruction_conditions[4]\n",
    "        self.reconstruction_conditions[2] = self.NOS_per_section + 1\n",
    "        self.learning_rate_corrected = False\n",
    "\n",
    "        # # Pack conditions into a list\n",
    "        # conditions = [noise, delta_T, NOS, theta_degrees, NOS_per_section, SRD, RDD,method,dataPiling]\n",
    "     # input format, list of tuple of two elements (x,y)\n",
    "\n",
    "    def assign_particle_id(self):\n",
    "        returnID = self.particle_id\n",
    "        self.particle_id += 1\n",
    "        return returnID\n",
    "    \n",
    "    def append(self, snapshot):\n",
    "        # snapshot = snapshot.tolist()\n",
    "        # store the snapshot in a time sequence dictionary\n",
    "        # print(\"current_snapShotIndex: \",self.current_snapShotIndex)\n",
    "        # print(\"current shot: \",snapshot)\n",
    "        # if self.current_snapShotIndex != 0:\n",
    "        #     print(\"previous shot: \", self.shotData[self.current_snapShotIndex - 1])\n",
    "\n",
    "    \n",
    "        if self.current_snapShotIndex not in self.shotData:\n",
    "            self.shotData[self.current_snapShotIndex] = snapshot\n",
    "\n",
    "        if self.current_snapShotIndex not in self.original_shotData:\n",
    "            self.original_shotData[self.current_snapShotIndex] = snapshot.copy()\n",
    "        \n",
    "        \n",
    "        if self.current_snapShotIndex == 0:\n",
    "            self.save_initial_particles(snapshot)\n",
    "        else:\n",
    "            # match previous particles to current\n",
    "\n",
    "            self.match_previous_particle_to_current(snapshot)\n",
    "\n",
    "        # print(\"particleData: \",self.particleData)\n",
    "\n",
    "        self.current_snapShotIndex += 1\n",
    "\n",
    "    def correct_learning_rate(self, learning_rates_data: list):\n",
    "        for i in range(len(learning_rates_data)):\n",
    "            self.corrected_learning_rates_data[i] = learning_rates_data[i]\n",
    "        self.learning_rate_corrected = True\n",
    " \n",
    "    def find_array_in_list(self,target, list_of_arrays):\n",
    "        for idx, arr in enumerate(list_of_arrays):\n",
    "            # print(\"equal between: \",target, \" and \", arr, \" is: \",np.array_equal(target, arr))\n",
    "            if np.array_equal(target, arr):\n",
    "\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def find_relative_snapshotIndex(self, particle_id, snapshot_index):\n",
    "        if particle_id in self.particleData_2D:\n",
    "            data = self.particleData_2D[particle_id]\n",
    "\n",
    "            # print(\"data with particle id, \",particle_id, \" is: \",data)\n",
    "            # print(\"snapshot_index: \",snapshot_index)\n",
    "            # if self.current_snapShotIndex > 141:\n",
    "            #     print(self.particleData_2D)\n",
    "            \n",
    "            # Check if the given snapshot index exists in the list of snapshot indices\n",
    "            if snapshot_index in data['snapshotIndexList']:\n",
    "                # Find the index of the given snapshot index in the list\n",
    "                index = data['snapshotIndexList'].index(snapshot_index)\n",
    "                \n",
    "\n",
    "                return index\n",
    "\n",
    "        # Return None if the particle or snapshot index is not found\n",
    "        KeyError(\"particle_id or snapshot_index not found\")\n",
    "        return None\n",
    "\n",
    "    def find_closest_particle(self, particle, shot, closest_rank=1):\n",
    "\n",
    "        distances = np.linalg.norm(shot - particle, axis=1)\n",
    "\n",
    "        minDistanceIndex = np.argmin(distances)\n",
    "\n",
    "        if closest_rank > 1:\n",
    "            for i in range(closest_rank - 1):\n",
    "                minDistanceIndex = np.argmin(np.delete(distances, minDistanceIndex))\n",
    "        return minDistanceIndex\n",
    "\n",
    "    def get_default_learning_rate(self):\n",
    "        return self.learning_rate_2D\n",
    "\n",
    "    def get_coordinates_by_snapshot(self, particle_id, snapshot_index):\n",
    "        if particle_id in self.particleData_2D:\n",
    "            data = self.particleData_2D[particle_id]\n",
    "            \n",
    "            # Check if the given snapshot index exists in the list of snapshot indices\n",
    "            if snapshot_index in data['snapshotIndexList']:\n",
    "                # Find the index of the given snapshot index in the list\n",
    "                index = data['snapshotIndexList'].index(snapshot_index)\n",
    "                \n",
    "                # Use the index to access the coordinates\n",
    "                coordinates = data['coords'][index]\n",
    "                return coordinates\n",
    "\n",
    "        # Return None if the particle or snapshot index is not found\n",
    "        return None\n",
    "\n",
    "    def get_original_shotData(self):\n",
    "        return self.original_shotData\n",
    "\n",
    "    def get_particle_id(self, particle,snapshotID, closest_rank=1,tolerance=0.01):\n",
    "        target_snapshot = self.shotData[snapshotID]\n",
    "        closest_particle_id_in_shot = self.find_closest_particle(particle, np.array(self.shotData[snapshotID]),closest_rank)\n",
    "\n",
    "        # print(\"particle in get particle id: \",particle)\n",
    "        # print(\"closest_particle_coor: \",closest_particle_id_in_shot)\n",
    "        # print(\"target_snapshot: \",target_snapshot)\n",
    "        # print(\"particleData with id: \",self.particleData[closest_particle_id_in_shot])\n",
    "        # print(\"snapshotID relative: \",particle_relative_shotID)\n",
    "        # print(\"particleData on this shot: \",self.particleData[closest_particle_id_in_shot]['coords'][particle_relative_shotID])\n",
    "        for particle_id in self.particleData_2D:\n",
    "\n",
    "            # print(\"iterating at particle_id: \",particle_id)\n",
    "            # print(\"particleData_individual: \", self.particleData[particle_id]['coords'][snapshotID])\n",
    "            # print(\"target_snapshot[closest_particle_id_in_shot]: \",target_snapshot[closest_particle_id_in_shot])\n",
    "            # print(\"self.particleData[particle_id]['coords'][snapshotID]: \",self.particleData[particle_id]['coords'])\n",
    "\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            distance = np.linalg.norm(target_snapshot[closest_particle_id_in_shot] - self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "            # print(\"particle_relative_shotID: \",particle_relative_shotID)    \n",
    "            if distance < tolerance:\n",
    "                # print(\"found the particle id: \",particle_id)\n",
    "                # particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "                # print(\"particle_relative_shotID: \",particle_relative_shotID)\n",
    "                print(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "            \n",
    "                return particle_id, self.particleData_2D[particle_id]['coords'][particle_relative_shotID]\n",
    "            \n",
    "\n",
    "                \n",
    "        print(\"not found\")\n",
    "        print(\"orignal particle: \", target_snapshot[closest_particle_id_in_shot], \" minus: \", self.particleData_2D['coords'][-1])\n",
    "        KeyError(\"particle_id not found\")\n",
    "\n",
    "    def get_particle_id_from_available_ids(self,particle, snapshotID,id_list):\n",
    "        target_coordinates = []\n",
    "        \n",
    "        for particle_id in id_list:\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            target_coordinates.append(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "        closest_particle_id_in_shot = id_list[self.find_closest_particle(particle, np.array(target_coordinates))]\n",
    "        return closest_particle_id_in_shot, self.particleData_2D[closest_particle_id_in_shot]['coords'][particle_relative_shotID]\n",
    "\n",
    "    def get_particle_id_from_unmatched_ids(self,particle, snapshotID,matched_id_list):\n",
    "        id_list = list(range(0, len(self.particleData_2D)))\n",
    "        for id in matched_id_list:\n",
    "            id_list.remove(id)\n",
    "        target_coordinates = []\n",
    "        for particle_id in id_list:\n",
    "            particle_relative_shotID = self.find_relative_snapshotIndex(particle_id, snapshotID)\n",
    "            # print(\"particle_relative_shotID: \",particle_relative_shotID)\n",
    "            target_coordinates.append(self.particleData_2D[particle_id]['coords'][particle_relative_shotID])\n",
    "        # print(\"target_coordinates: \",target_coordinates)\n",
    "        closest_particle_id_in_shot = id_list[self.find_closest_particle(particle, np.array(target_coordinates))]\n",
    "        \n",
    "        return closest_particle_id_in_shot, self.particleData_2D[closest_particle_id_in_shot]['coords'][particle_relative_shotID]\n",
    "\n",
    "    def get_total_num_of_particles(self):\n",
    "        return len(self.particleData_2D)\n",
    "\n",
    "    def get_particle_data(self):\n",
    "        return self.particleData_2D\n",
    "\n",
    "    def historicalLinearVelocity(self, previous_particle_id, num_of_snapshots_to_check, discount_factor=1, direction=\"backward\"):\n",
    "        last10Coordiantes = []\n",
    "\n",
    "        for i in range (1,num_of_snapshots_to_check + 1):\n",
    "            last10Coordiantes.append(self.particleData_2D[previous_particle_id]['coords'][self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - i)])\n",
    "                        # k = k**2\n",
    "                        \n",
    "        velocity_array = np.diff(np.array(last10Coordiantes), axis=0)\n",
    "        final_velocity = np.zeros(2)\n",
    "        k = 0\n",
    "\n",
    "        for i in range(len(velocity_array)):\n",
    "            if direction == \"backward\":\n",
    "                final_velocity += velocity_array[i] * discount_factor**(i+1)\n",
    "            elif direction == \"forward\":\n",
    "                final_velocity += velocity_array[len(velocity_array)-1-i] * discount_factor**(i+1)\n",
    "            else:\n",
    "                # assume backward\n",
    "                final_velocity += velocity_array[i] * discount_factor**(i+1)\n",
    "            k += discount_factor**(i+1)\n",
    "        return final_velocity/k\n",
    "\n",
    "    def match_previous_particle_to_current(self, current_shot):\n",
    "\n",
    "        def is_motion_random(historical_vel, observed_vel, motion_randomness):\n",
    "            threshold = 1e-6\n",
    "            is_random = False\n",
    "            if np.isscalar(historical_vel):\n",
    "                length_of_historical_vel = 1\n",
    "            else:\n",
    "                length_of_historical_vel = len(historical_vel)\n",
    "    \n",
    "            for i in range(length_of_historical_vel):\n",
    "                if np.isscalar(observed_vel):\n",
    "                    compare_observe_vel = observed_vel\n",
    "                else:\n",
    "                    compare_observe_vel = observed_vel[i]\n",
    "\n",
    "                if np.isscalar(historical_vel):\n",
    "                    compare_historical_vel = historical_vel\n",
    "                else:\n",
    "                    compare_historical_vel = historical_vel[i]\n",
    "\n",
    "                if compare_historical_vel <= threshold:\n",
    "                    # stationary, no division\n",
    "                    x = abs(compare_historical_vel - compare_observe_vel) > motion_randomness\n",
    "                else:\n",
    "                    x = abs(compare_historical_vel - compare_observe_vel) > motion_randomness\n",
    "                \n",
    "                if x:\n",
    "                    is_random = True\n",
    "                    break\n",
    "\n",
    "            return is_random\n",
    "\n",
    "        previous_shot = self.shotData[self.current_snapShotIndex - 1]\n",
    "\n",
    "        # create defensive copies of the previous and current shots so we can delete items to keep track without affecting the original data\n",
    "        previous_shot_remain = previous_shot.copy()\n",
    "        \n",
    "        current_shot_remain = current_shot.copy()\n",
    "\n",
    "        \n",
    "\n",
    "        # get the ranked list of particles\n",
    "        ranked_particle_list = self.rank_particle_distances(previous_shot_remain, current_shot_remain, search_radius=10)\n",
    "\n",
    "        matched_particles_id = []\n",
    "\n",
    "\n",
    "        # while there are still particles unmatched, we keep matching\n",
    "        while len(previous_shot_remain) > 0 and len(current_shot_remain) > 0:\n",
    "\n",
    "            # get the closest particle\n",
    "            closest_particles = heapq.heappop(ranked_particle_list)\n",
    "       \n",
    "            previous_index = closest_particles[2]\n",
    "            current_index = closest_particles[1]\n",
    "            current_particle_to_match = current_shot[current_index]\n",
    "            print(\" \")\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"current_snapshot: \",self.current_snapShotIndex)\n",
    "            # [previous_particle_id, prev_particle_coor] = self.get_particle_id(previous_shot[previous_index], self.current_snapShotIndex - 1)\n",
    "            # print(\"particleData: \",self.particleData)\n",
    "            if matched_particles_id is None:\n",
    "                [previous_particle_id, prev_particle_coor] = self.get_particle_id(previous_shot[previous_index], self.current_snapShotIndex - 1)\n",
    "            else:\n",
    "                [previous_particle_id, prev_particle_coor] = self.get_particle_id_from_unmatched_ids(previous_shot[previous_index], self.current_snapShotIndex - 1,matched_particles_id)\n",
    "            # delete the points that are matched from the defensive copies\n",
    "            if not self.find_array_in_list(prev_particle_coor, previous_shot_remain) or not self.find_array_in_list(current_particle_to_match, current_shot_remain):\n",
    "                # print(\"skip this occurance \\n \\n\")\n",
    "                continue\n",
    "            \n",
    "            # delete the points that are matched from the defensive copies\n",
    "            for idx, particle in enumerate(previous_shot_remain):\n",
    "                if np.array_equal(particle, prev_particle_coor):\n",
    "                    del previous_shot_remain[idx]\n",
    "                    break\n",
    "          \n",
    "            # explicit loop to remove the element from the current shot list\n",
    "            for idx, particle in enumerate(current_shot_remain):\n",
    "                if np.array_equal(particle, current_particle_to_match):\n",
    "                    del current_shot_remain[idx]\n",
    "                    break\n",
    "            \n",
    "            print(\"we are matching: \",prev_particle_coor, \" with \", current_particle_to_match, \" particle id: \",previous_particle_id)\n",
    "\n",
    "\n",
    "            if previous_particle_id not in self.particleData_2D:\n",
    "                self.particleData_2D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set(), 'learning_rate': []}\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            if self.current_snapShotIndex > 10:\n",
    "                \n",
    "\n",
    "                estimated_vel_from_historical_velocity = self.historicalLinearVelocity(previous_particle_id, 10, discount_factor=0.9)\n",
    "                \n",
    "                observed_vel = current_shot[current_index] - previous_shot[previous_index]\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                if is_motion_random(estimated_vel_from_historical_velocity, observed_vel, self.motion_randomness):\n",
    "\n",
    "                    print(\"motion randomness detected in particle: \",previous_particle_id)\n",
    "                    # motion randomness is too high, we then conduct a reconstruction to verify if the point is valid\n",
    "                    # 3D reconstruction and re sorting strategy \n",
    "                    previous_relative_index = self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - 1)\n",
    "\n",
    "                    compensation_3D = False\n",
    "                    # make sure we have enough data to do the reconstruction\n",
    "                    if previous_relative_index - self.NOS_per_section > 8 and compensation_3D == True:\n",
    "\n",
    "                        # take the previous NOS_per_section data to do the reconstruction for comparison\n",
    "\n",
    "                        # print()\n",
    "\n",
    "                        previous_2D_shots_selected = self.particleData_2D[previous_particle_id]['coords'][previous_relative_index - self.NOS_per_section : previous_relative_index]\n",
    "                        print(\"length of previous_2D_shots_selected: \",len(previous_2D_shots_selected))\n",
    "\n",
    "                        # \n",
    "                        self.reconstruction_conditions[2] = self.NOS\n",
    "                        previous_estimated_positions_single = Phase4_trace_3d(self.reconstruction_conditions, np.array(self.particleData_2D[previous_particle_id]['coords'][:]))\n",
    "\n",
    "                        self.reconstruction_conditions[2] = self.NOS_per_section + 1\n",
    "                        # take the previous NOS_per_section data and the current shot to be added to do the reconstruction and compare\n",
    "                        current_estimated_positions_single = Phase4_trace_3d(self.reconstruction_conditions, np.row_stack([np.array(self.particleData_2D[previous_particle_id]['coords'][previous_relative_index - self.NOS_per_section + 1 : previous_relative_index + 1]), current_particle_to_match]))\n",
    "\n",
    "                        learning_rate_3D = self.learning_rate_3D\n",
    "                        exploitation_rate_3D = 1 - learning_rate_3D\n",
    "\n",
    "                        # get observation velocity from the reconstruction\n",
    "                        observed_vel_3D = current_estimated_positions_single[-1,:] - current_estimated_positions_single[-2,:]\n",
    "\n",
    "                        # take the last 10 data points to do the historical velocity estimation\n",
    "                        # take the last 10 rows and all columns\n",
    "                        historical_vel_3D = np.mean(np.diff(previous_estimated_positions_single[-10:,:], axis=0))\n",
    "        \n",
    "\n",
    "                        # re-calculate the final position if adjustment is needed\n",
    "                        adjusted_vel_3D = observed_vel_3D\n",
    "                        j = 0\n",
    "                        for vel in adjusted_vel_3D:\n",
    "                            if is_motion_random(historical_vel_3D, observed_vel_3D[j], self.motion_randomness):\n",
    "                                adjusted_vel_3D[j] = historical_vel_3D * exploitation_rate_3D + vel * learning_rate_3D\n",
    "\n",
    "                            j += 1\n",
    "\n",
    "                        adjusted_position_3D = current_estimated_positions_single[-1] + adjusted_vel_3D\n",
    "                        adjusted_position_2D = self.particle_projection(self.alpha, adjusted_position_3D)\n",
    "\n",
    "                        # # now update the estimated position\n",
    "                        # if previous_particle_id not in self.particleData_3D:\n",
    "                        #     self.particleData_3D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set()}\n",
    "                        #     self.particleData_3D[previous_particle_id]['coords'].append(current_estimated_positions_single[:-1])\n",
    "                        #     self.particleData_3D[previous_particle_id]['snapshotIndexList'] = self.particleData_2D[previous_particle_id]['snapshotIndexList']\n",
    "                        #     self.particleData_3D[previous_particle_id]['snapshotIndexSet'] = self.particleData_2D[previous_particle_id]['snapshotIndexSet']\n",
    "                            \n",
    "                            \n",
    "                        # self.particleData_3D[previous_particle_id]['coords'].append(current_estimated_positions_single[-1] + adjusted_vel_3D)\n",
    "\n",
    "                        # # add the new positions with the old, and then take average\n",
    "                        # new_positions[:len(last_positions), :] = (new_positions[:len(last_positions), :] + last_positions) / 2\n",
    "                        # positions_predicted[proj_used_index:proj_used_index + new_positions.shape[0], :] = new_positions\n",
    "                        print(\"adjusted_position_2D: \",adjusted_position_2D)\n",
    "                        final_xy = adjusted_position_2D\n",
    "\n",
    " \n",
    "\n",
    "                    else:\n",
    "                        print(\"not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\")\n",
    "                        # calculate position from learning factor\n",
    "                        if self.learning_rate_corrected:\n",
    "                            learning_rates_2D = self.corrected_learning_rates_data[previous_particle_id]\n",
    "                            learning_rate_2D = learning_rates_2D[self.current_snapShotIndex]\n",
    "                        else:\n",
    "                            learning_rate_2D = self.learning_rate_2D\n",
    "                        exploitation_rate_2D = 1 - learning_rate_2D\n",
    "                        final_xy = estimated_vel_from_historical_velocity*exploitation_rate_2D + observed_vel*learning_rate_2D + previous_shot[previous_index]\n",
    "                        print(\"final_xy: \", final_xy)\n",
    "                    \n",
    "                    self.particleData_2D[previous_particle_id]['coords'].append(final_xy)\n",
    "                        # modify shot data to keep the consistency\n",
    "                    self.shotData[self.current_snapShotIndex][current_index] = final_xy\n",
    "                \n",
    "                else:\n",
    "                    self.particleData_2D[previous_particle_id]['coords'].append(current_particle_to_match)\n",
    "                \n",
    "            else:\n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(current_particle_to_match)\n",
    "\n",
    "    \n",
    "            self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "            \n",
    "            self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "\n",
    "            matched_particles_id.append(previous_particle_id)\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        # print(\"Entered compensation mode\")\n",
    "        # if the new snapshot has more particles than the previous one by comparing the length of the remaining particles in the defensive copies\n",
    "        if len(current_shot_remain) > len(previous_shot_remain):\n",
    "            print(\"Entered compensation mode, more current particles than previous\")\n",
    "            # print(\"current_shot_remain: \",current_shot_remain)\n",
    "            # create new unique particles and save them \n",
    "            for particle in current_shot_remain:\n",
    "                previous_particle_id = self.assign_particle_id()\n",
    "                if previous_particle_id not in self.particleData_2D:\n",
    "                    self.particleData_2D[previous_particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set()}\n",
    "                    \n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(particle)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "\n",
    "        # if the new snapshot has less particles than the previous one\n",
    "        elif len(current_shot_remain) < len(previous_shot_remain):\n",
    "            print(\"Entered compensation mode: more previous particles than current\")\n",
    "            # print(\"previous_shot_remain: \",previous_shot_remain)\n",
    "            # we estimate the unmatched particle with the trajectory of the closest neighbor (current snapshot position - previous snapshot position)\n",
    "            \n",
    "            for prev_particle in previous_shot_remain:\n",
    "\n",
    "                previous_particle_id, prev_particle_coor = self.get_particle_id_from_unmatched_ids(prev_particle_coor, self.current_snapShotIndex-1,matched_particles_id)\n",
    "                \n",
    "                # neighbor strategy\n",
    "                if self.current_snapShotIndex <= 10:\n",
    "                    # print(\"id list: \",matched_particles_id)\n",
    "                    closest_neighbor_particle_id, closest_neighbor_previous_xy = self.get_particle_id_from_available_ids(prev_particle, self.current_snapShotIndex - 1, matched_particles_id)\n",
    "                    # print(\"closest_neighbor_particle_id: \",closest_neighbor_particle_id)\n",
    "                    relativeIndex = self.find_relative_snapshotIndex(closest_neighbor_particle_id, self.current_snapShotIndex)\n",
    "                    # print(relativeIndex)\n",
    "                    closest_neighbor_current_xy = self.particleData_2D[closest_neighbor_particle_id]['coords'][relativeIndex]\n",
    "                    # print(\"closest_neighbor_current_xy: \",closest_neighbor_current_xy)\n",
    "                    # closest_neighbor_previous_xy = self.get_coordinates_by_snapshot(closest_neighbor_particle_id, self.current_snapShotIndex - 1)\n",
    "                    # Calculate the difference between current and previous coordinates (c-p)\n",
    "                    difference_xy = np.array(closest_neighbor_current_xy) - np.array(closest_neighbor_previous_xy)\n",
    "\n",
    "                    estiamted_xy = tuple(np.array(prev_particle_coor) + difference_xy)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    previous_xy = self.particleData_2D[previous_particle_id]['coords'][self.find_relative_snapshotIndex(previous_particle_id, self.current_snapShotIndex - 1)]\n",
    "                    # historical velocity strategy\n",
    "                    estiamted_xy = self.historicalLinearVelocity(previous_particle_id,10,0.8) + previous_xy\n",
    "\n",
    "                self.particleData_2D[previous_particle_id]['coords'].append(estiamted_xy)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "                self.particleData_2D[previous_particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "                self.shotData[self.current_snapShotIndex].append(np.array(estiamted_xy))\n",
    "\n",
    "    def particle_projection(self, alpha, r_0):\n",
    "        r_0_rotated=rotation(r_0,alpha)\n",
    "        _, _, _, _, _, SOD, ODD,_,_ = self.reconstruction_conditions\n",
    "        M_p = (SOD + ODD) / (SOD + r_0_rotated[1])\n",
    "        \n",
    "    \n",
    "        return np.array([M_p * r_0_rotated[0], M_p * r_0_rotated[2]])\n",
    "\n",
    "    def save_initial_particles(self, snapshot):\n",
    "        for particle in snapshot:\n",
    "            particle_id = self.assign_particle_id()\n",
    "            # if self.paricleData is None:\n",
    "            #     self.paricleData = {particle_id: particle}\n",
    "            if particle_id not in self.particleData_2D:\n",
    "                self.particleData_2D[particle_id] = {'coords': [], 'snapshotIndexList': [], 'snapshotIndexSet': set(), 'learning_rate':[]}\n",
    "                \n",
    "            self.particleData_2D[particle_id]['coords'].append(particle)\n",
    "            self.particleData_2D[particle_id]['snapshotIndexList'].append(self.current_snapShotIndex)\n",
    "            self.particleData_2D[particle_id]['snapshotIndexSet'].add(self.current_snapShotIndex)\n",
    "            self.particleData_2D[particle_id]['learning_rate'].append(learning_rate_2D)\n",
    "\n",
    "    def rank_particle_distances(self, previous_snapshot, current_snapshot, search_radius):\n",
    "        # turn previous snapshot into a N by 2 matrix\n",
    "        previous_snapshot = np.array(previous_snapshot).reshape(-1,2)\n",
    "\n",
    "        # grab the x axis\n",
    "        previous_x = previous_snapshot[:,0]\n",
    "        # grab the y axis\n",
    "        previous_y = previous_snapshot[:,1]\n",
    "\n",
    "        # turn current snapshot into a N by 2 matrix\n",
    "        current_snapshot = np.array(current_snapshot).reshape(-1,2)\n",
    "\n",
    "        # grab the x axis\n",
    "        current_x = current_snapshot[:,0]\n",
    "        # grab the y axis\n",
    "        current_y = current_snapshot[:,1]\n",
    "\n",
    "        # get the large matrix to fix the case where there are different number of particles in the previous and current snapshot\n",
    "        x_large_matrix = np.tile(previous_x, (len(current_x), 1)) - np.tile(current_x, (len(previous_x), 1)).T\n",
    "        y_large_matrix = np.tile(previous_y, (len(current_y), 1)) - np.tile(current_y, (len(previous_y), 1)).T\n",
    "\n",
    "        # print(\"x_large_matrix: \",x_large_matrix)\n",
    "        # print(\"y_large_matrix: \",y_large_matrix)\n",
    "\n",
    "        # get the distance matrix\n",
    "        distance_matrix = np.sqrt(x_large_matrix**2 + y_large_matrix**2)\n",
    "        # print(\"distance_matrix: \",distance_matrix)\n",
    "\n",
    "        # create a heap queue to store the ranked particles\n",
    "        ranked_particle_heapq = []\n",
    "\n",
    "        # Get the sorted indices of the flattened distance_matrix\n",
    "        sorted_indices = np.argsort(distance_matrix.ravel())\n",
    "\n",
    "        # Convert the flattened indices to 2D row and column indices\n",
    "        row_col_indices = np.unravel_index(sorted_indices, distance_matrix.shape)\n",
    "\n",
    "        # Print the values in distance_matrix in ascending order along with their row and column indices\n",
    "        for i in range(len(sorted_indices)):\n",
    "            row, col = row_col_indices[0][i], row_col_indices[1][i]\n",
    "            # print(f\"Value: {distance_matrix[row, col]}, Row: {row}, Col: {col}\")\n",
    "            distance = distance_matrix[row, col]\n",
    "            # store the closest particles together. row is the index of the curret particle, col is the index of the previous particle\n",
    "            heapq.heappush(ranked_particle_heapq, (distance, row, col))\n",
    "            # print(\"original ranked_particle_heapq: \",len(ranked_particle_heapq))\n",
    "\n",
    "        return ranked_particle_heapq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D reconstruction functions\n",
    "\n",
    "def generateEstimatedPositions(alpha, proj_used_index, N, xz_proj, conditions):\n",
    "    _,delta_T, _, theta_degree, _, SOD, ODD,method,dataPiling = conditions\n",
    "    theta = np.deg2rad(theta_degree)\n",
    "    positions_predicted = np.zeros((N, 3))\n",
    "    \n",
    "    #     # Record the start time\n",
    "    # start_time = time.time() \n",
    "\n",
    "    # use the new method, more time, more accuracy\n",
    "    # values_this_round = proj2r0_acc(xz_proj[proj_used_index : proj_used_index+N-2, :], theta, SRD, RDD, delta_T)\n",
    "    # position_rotated = position_rotated[0]\n",
    "    # velocity_rotated = velocity_rotated[0]\n",
    "    # acc_rotated = acc_rotated[0]\n",
    "\n",
    "    # old method for time efficiency\n",
    "\n",
    "    print(\"proj_used_index: \",proj_used_index)\n",
    "    print(proj_used_index+N-2)\n",
    "    values_this_round = proj2r0_acc_old(xz_proj[proj_used_index : proj_used_index+N-2, :], theta, SOD, ODD, delta_T)\n",
    "        # Record the end time\n",
    "    # end_time = time.time()\n",
    "\n",
    "    # # Calculate and print the total runtime\n",
    "    # runtime = end_time - start_time\n",
    "    # print(f\"The runtime of projr20 is {runtime} seconds.\")\n",
    "    if method == 'acceleration':\n",
    "        x0, y0, z0, u, v, w, a_x, a_y, a_z = values_this_round\n",
    "        # print(\"values_this_round\",values_this_round)\n",
    "        position_rotated =  np.transpose(rotation([x0, y0, z0], alpha))\n",
    "        # print(\"position_rotated\",position_rotated)\n",
    "        x0, y0, z0 = position_rotated[0][0], position_rotated[0][1], position_rotated[0][2]\n",
    "        positions_predicted[0, :] = position_rotated\n",
    "        velocity_rotated = np.transpose(rotation([u, v, w], alpha))\n",
    "        u, v, w = velocity_rotated[0][0], velocity_rotated[0][1], velocity_rotated[0][2]\n",
    "        acc_rotated =  np.transpose(rotation([a_x, a_y, a_z], alpha))\n",
    "        a_x, a_y, a_z = acc_rotated[0][0], acc_rotated[0][1], acc_rotated[0][2]\n",
    "\n",
    "\n",
    "        for j in range(1, N):\n",
    "            time = delta_T * (j)\n",
    "            positions_predicted[j, :] = [x0+u*time+0.5*a_x*time**2, y0+v*time+0.5*a_y*time**2, z0+w*time+0.5*a_z*time**2]\n",
    "\n",
    "\n",
    "    elif method == 'linear':\n",
    "        x0, y0, z0, u, v, w = values_this_round\n",
    "        position_rotated = np.transpose(rotation([x0, y0, z0], alpha))\n",
    "        x0, y0, z0 = position_rotated\n",
    "        positions_predicted[0, :] = position_rotated\n",
    "        velocity_rotated = np.transpose(rotation([u, v, w], alpha))\n",
    "        u, v, w = velocity_rotated\n",
    "\n",
    "        for j in range(1, N):\n",
    "            time = delta_T * (j)\n",
    "            positions_predicted[j, :] = [x0+u*time, y0+v*time, z0+w*time]\n",
    "\n",
    "    return positions_predicted   \n",
    "\n",
    "def proj2r0_acc_old(proj, theta, SOD, ODD, delta_T):\n",
    "    NOS = len(proj)\n",
    "    print(proj)\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = 2 * NOS + 2 * (NOS - 1)\n",
    "    col_number_A = 1 + 2 * NOS\n",
    "    print(\"row_number_A: \",row_number_A)\n",
    "    print(\"col_number_A: \",col_number_A)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A,1))\n",
    "    \n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = proj[j]\n",
    "        A[2*j, 0] = 1\n",
    "        A[2*j, 2*j+2] = -zi_j / SDD\n",
    "        b[2*j] = zi_j * SOD / SDD\n",
    "        \n",
    "        A[2*j+1, 2*j+1] = -1\n",
    "        A[2*j+1, 2*j+2] = xi_j / SDD\n",
    "        b[2*j+1] = -xi_j * SOD / SDD\n",
    "    \n",
    "    x = 2 * NOS\n",
    "    for k in range(1, NOS):\n",
    "        A[x:x+2, 2*k+1:2*k+3] = [[-1, 0], [0, -1]]\n",
    "        A[x:x+2, 1:3] = [[np.cos(theta*(k)), np.sin(theta*(k))], [-np.sin(theta*(k)), np.cos(theta*(k))]]\n",
    "        x += 2\n",
    "        \n",
    "    A = np.pad(A, ((0, 0), (0, 6)), 'constant')\n",
    "    new_col_num = A.shape[1]\n",
    "    \n",
    "    for j in range(NOS):\n",
    "        A[2*j, new_col_num-4] = delta_T * (j)\n",
    "        A[2*j, new_col_num-1] = 0.5 * (delta_T * (j))**2\n",
    "        \n",
    "    IoR = 2 * NOS\n",
    "    for k in range(1, NOS):\n",
    "        A[IoR, new_col_num-6] = np.cos(theta * k) * delta_T * k\n",
    "        A[IoR+1, new_col_num-6] = -np.sin(theta * k) * delta_T * k\n",
    "        \n",
    "        A[IoR, new_col_num-5] = np.sin(theta * k) * delta_T * k\n",
    "        A[IoR+1, new_col_num-5] = np.cos(theta * k) * delta_T * k\n",
    "        \n",
    "        A[IoR, new_col_num-3] = 0.5 * np.cos(theta * k) * (delta_T * k)**2\n",
    "        A[IoR+1, new_col_num-3] = -np.sin(theta * k) * 0.5 * (delta_T * k)**2\n",
    "        \n",
    "        A[IoR, new_col_num-2] = 0.5 * np.sin(theta * k) * (delta_T * k)**2\n",
    "        A[IoR+1, new_col_num-2] = 0.5 * np.cos(theta * k) * (delta_T * k)**2\n",
    "        IoR += 2\n",
    "        \n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 =[x[1], x[2], x[0], x[new_col_num-6], x[new_col_num-5], x[new_col_num-4], x[new_col_num-3], x[new_col_num-2], x[new_col_num-1]]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_acc(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    "    A = np.hstack([A, np.zeros((A.shape[0], 6))])\n",
    "    new_col_num = A.shape[1]\n",
    "    u_ind, v_ind, w_ind, ax_ind, ay_ind, az_ind = new_col_num - 6, new_col_num - 5, new_col_num - 4, new_col_num - 3, new_col_num - 2, new_col_num - 1\n",
    "\n",
    "    for j in range(NOS):\n",
    "        A[2 * j, w_ind] = dt * j\n",
    "        A[2 * j, az_ind] = 0.5 * (dt * j)**2\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        T2 = dt * k\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            T1 = T2 - (i+1) * dt\n",
    "            theta_prime = theta * (k - i - 1)\n",
    "            A[IoR, u_ind] = (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, u_ind] = (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, v_ind] = (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, v_ind] = (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, ax_ind] = 0.5 * (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR + 1, ax_ind] = 0.5 * (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR, ay_ind] = 0.5 * (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2**2 - T1**2)\n",
    "            A[IoR + 1, ay_ind] = 0.5 * (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2**2 - T1**2)\n",
    "            IoR += 2\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], x[u_ind], x[v_ind], x[w_ind], x[ax_ind], x[ay_ind], x[az_ind]]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_vel(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    "    A = np.hstack([A, np.zeros((A.shape[0], 3))])\n",
    "    new_col_num = A.shape[1]\n",
    "    u_ind, v_ind, w_ind = new_col_num - 3, new_col_num - 2, new_col_num - 1\n",
    "\n",
    "    for j in range(NOS):\n",
    "        A[2 * j, w_ind] = dt * j\n",
    "        \n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        T2 = dt * k\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            T1 = T2 - (i+1) * dt\n",
    "            theta_prime = theta * (k - i - 1)\n",
    "            A[IoR, u_ind] = (np.cos(theta_prime) * np.cos(delta_theta) - np.sin(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, u_ind] = (-np.cos(theta_prime) * np.sin(delta_theta) - np.sin(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            A[IoR, v_ind] = (np.sin(theta_prime) * np.cos(delta_theta) + np.cos(theta_prime) * np.sin(delta_theta)) * (T2 - T1)\n",
    "            A[IoR + 1, v_ind] = (-np.sin(theta_prime) * np.sin(delta_theta) + np.cos(theta_prime) * np.cos(delta_theta)) * (T2 - T1)\n",
    "            IoR += 2\n",
    "\n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], x[u_ind], x[v_ind], x[w_ind], 0,0,0]\n",
    "    return r0\n",
    "\n",
    "def proj2r0_stationary(xz_proj, theta, SOD, ODD, dt):\n",
    "    NOS = xz_proj.shape[0]\n",
    "    SDD = SOD + ODD\n",
    "    row_number_A = int(2 * NOS + 2 * (np.math.factorial(NOS) / (np.math.factorial(NOS - 2) * 2)))\n",
    "    col_number_A = int(1 + 2 * NOS)\n",
    "    A = np.zeros((row_number_A, col_number_A))\n",
    "    b = np.zeros((row_number_A, 1))\n",
    "\n",
    "    for j in range(NOS):\n",
    "        xi_j, zi_j = xz_proj[j]\n",
    "        A[2 * j, 0] = 1\n",
    "        A[2 * j, 2 * j +2] = -zi_j / SDD\n",
    "        b[2 * j] = zi_j * SOD / SDD\n",
    "        A[2 * j + 1, 2 * j+1] = -1\n",
    "        A[2 * j + 1, 2 * j + 2] = xi_j / SDD\n",
    "        b[2 * j + 1] = -xi_j * SOD / SDD\n",
    "\n",
    "    IoR = 2 * NOS\n",
    "\n",
    "    for k in range(1, NOS):\n",
    "        trans_count = k\n",
    "        A[IoR:IoR + 2 * trans_count, 2 * k+1:2 * k + 3] = np.tile(np.array([[-1, 0], [0, -1]]), (trans_count, 1))\n",
    "\n",
    "        for i in range(trans_count):\n",
    "            delta_theta = theta * (i + 1)\n",
    "            A[IoR + 2 * i:IoR + 2 * i + 2, 2 * (k - 1) - 2 * i+1:2 * (k - 1) - 2 * i + 2+1] = np.array([[np.cos(delta_theta), np.sin(delta_theta)], [-np.sin(delta_theta), np.cos(delta_theta)]])\n",
    "        IoR += 2 * trans_count\n",
    "\n",
    " \n",
    "    x, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "    r0 = [x[1], x[2], x[0], 0,0,0, 0,0,0]\n",
    "    return r0\n",
    "\n",
    "def Phase4_trace_3d(conditions, xz_proj):\n",
    "    _,delta_T, NOS, theta_degree, N, SOD, ODD,method,dataPiling = conditions\n",
    "\n",
    "    theta = np.deg2rad(theta_degree)\n",
    "\n",
    "    proj_used_index = 0\n",
    "    \n",
    "    NOS = int(conditions[2])  # Convert to int before using\n",
    "    positions_predicted = np.zeros((NOS, 3))\n",
    "    \n",
    "    NOS_per_section = N\n",
    "    prev_NOS_section = NOS_per_section\n",
    "    print(\"NOS: \",NOS)\n",
    "    print(\"NOS_per_Section: \",NOS_per_section) \n",
    "\n",
    "    if dataPiling == 'serial':\n",
    "        while proj_used_index < NOS:\n",
    "            alpha = -theta*(proj_used_index)\n",
    "            temp = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "            positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = temp\n",
    "            # print(\"normal called\")\n",
    "\n",
    "            proj_used_index += NOS_per_section \n",
    "            # proj_used_index += 1\n",
    "\n",
    "            if abs(NOS - proj_used_index) < N:\n",
    "                # NOS_per_section = NOS - proj_used_index + 1\n",
    "                # alpha = -theta*(proj_used_index)\n",
    "                # temp = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section-1,xz_proj,conditions)\n",
    "                # positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = temp\n",
    "                # proj_used_index += NOS_per_section\n",
    "\n",
    "                prev_proj_index = proj_used_index\n",
    "                proj_used_index = NOS - NOS_per_section\n",
    "                alpha = -theta*(proj_used_index)\n",
    "                last_positions = positions_predicted[proj_used_index : prev_proj_index, :]\n",
    "                new_positions = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "                # print('prev',prev_proj_index)\n",
    "                # print('proj_index',proj_used_index)\n",
    "                # print('last positions',last_positions)\n",
    "                # print('new positions',new_positions)\n",
    "                combined_positions = np.concatenate([(new_positions[:len(last_positions), :] + last_positions)/2, new_positions[len(last_positions):, :]], axis=0)\n",
    "\n",
    "                positions_predicted[proj_used_index : proj_used_index+NOS_per_section, :] = combined_positions\n",
    "                # print(\"retro called\")\n",
    "                proj_used_index += NOS_per_section + 1\n",
    "                \n",
    "\n",
    "    elif dataPiling == 'overlap':\n",
    "        for i in range(round(NOS - N)):\n",
    "            print(\"iterations: \",i)\n",
    "            alpha = -theta * (proj_used_index - 1)  # alpha is for tracking the degree rotated from the 1st shot\n",
    "\n",
    "            if proj_used_index == 1:\n",
    "                positions_predicted[proj_used_index:N+1, :] = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "            else:\n",
    "                # take every N shots from every index, and take average of them\n",
    "                last_positions = positions_predicted[proj_used_index: proj_used_index + prev_NOS_section - 1, :]\n",
    "\n",
    "                \n",
    "                new_positions = generateEstimatedPositions(alpha, proj_used_index, NOS_per_section,xz_proj,conditions)\n",
    "        \n",
    "\n",
    "\n",
    "                new_positions[:len(last_positions), :] = (new_positions[:len(last_positions), :] + last_positions) / 2\n",
    "                positions_predicted[proj_used_index:proj_used_index + new_positions.shape[0], :] = new_positions\n",
    "      \n",
    "\n",
    "            proj_used_index += 1\n",
    "\n",
    "\n",
    "    return positions_predicted\n",
    "\n",
    "def smooth_points(estimated_positions, method, frame_size):\n",
    "    \"\"\"\n",
    "    Smoothens the given 3D estimated positions using one of the specified methods.\n",
    "    \n",
    "    Parameters:\n",
    "    - estimated_positions (ndarray): Nx3 array of estimated 3D positions.\n",
    "    - method (str): Smoothing method ('avg', 'sg', or 'cb').\n",
    "    - frame_size (int): Window size for moving average or Savitzky-Golay filter.\n",
    "    \n",
    "    Returns:\n",
    "    - ndarray: Nx3 array of smoothened 3D positions.\n",
    "    \"\"\"\n",
    "    # Initialize the filtered measurements with the original data\n",
    "    filtered_measurements = estimated_positions.copy()\n",
    "    \n",
    "    # Moving Average\n",
    "    if method == 'avg':\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            filtered_measurements[:, i] = np.convolve(estimated_positions[:, i], np.ones(frame_size)/frame_size, mode='same')\n",
    "    \n",
    "    # Savitzky-Golay Filter\n",
    "    elif method == 'sg':\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            filtered_measurements[:, i] = savgol_filter(estimated_positions[:, i], frame_size, 2)\n",
    "    \n",
    "    # Cubic Smoothing Spline\n",
    "    elif method == 'cb':\n",
    "        x = np.arange(estimated_positions.shape[0])\n",
    "        for i in range(3):  # Loop over each dimension\n",
    "            spl = UnivariateSpline(x, estimated_positions[:, i], s=0.5)\n",
    "            filtered_measurements[:, i] = spl(x)\n",
    "    \n",
    "    return filtered_measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphing functions\n",
    "def number_to_binary_list(number):\n",
    "    binary_str = bin(number)[2:]  # Convert to binary and remove the '0b' prefix\n",
    "    binary_str = binary_str.zfill(3)  # Pad with zeros to make sure it has 3 digits\n",
    "    binary_list = np.array([int(b) for b in binary_str] ) # Convert each binary digit to integer\n",
    "    return binary_list\n",
    "\n",
    "def plotting_single(positions, particle_id,ax):\n",
    "    NOS, _ = positions.shape\n",
    "    particle_id=int(particle_id)+1\n",
    "    if particle_id<7 and particle_id>=0:\n",
    "        col=number_to_binary_list(int(particle_id))\n",
    "    else:\n",
    "        col=np.array([random.uniform(0.6, 1) for _ in range(3)])\n",
    "        print(col)\n",
    "\n",
    "    \n",
    "    # Plot the curve with gradually changing color\n",
    "    for i in range(NOS - 1):\n",
    "        color_brightness = (i/(NOS-1))*col\n",
    "        \n",
    "        x1, y1, z1 = positions[i]\n",
    "        x2, y2, z2 = positions[i+1]\n",
    "        \n",
    "        # Draw a line segment with the computed color\n",
    "        ax.plot([x1, x2], [y1, y2], [z1, z2], color=color_brightness)\n",
    "    ax.plot([x1, x2], [y1, y2], [z1, z2], color=color_brightness,label='particle'+str(particle_id-1))\n",
    "    # Label the axes\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file processing function\n",
    "def rename_files_replace_space(directory_path):\n",
    "    # Get a list of all files in the directory\n",
    "    filenames = os.listdir(directory_path)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        # Replace spaces with underscores\n",
    "        new_filename = filename.replace('_', '')\n",
    "        \n",
    "        # Construct the full old and new file paths\n",
    "        old_filepath = os.path.join(directory_path, filename)\n",
    "        new_filepath = os.path.join(directory_path, new_filename)\n",
    "        \n",
    "        # Rename the file\n",
    "        os.rename(old_filepath, new_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list of files:  ['Shot0.csv', 'Shot1.csv', 'Shot10.csv', 'Shot11.csv', 'Shot12.csv', 'Shot13.csv', 'Shot14.csv', 'Shot15.csv', 'Shot16.csv', 'Shot17.csv', 'Shot2.csv', 'Shot3.csv', 'Shot4.csv', 'Shot5.csv', 'Shot6.csv', 'Shot7.csv', 'Shot8.csv', 'Shot9.csv']\n",
      "['Shot0.csv', 'Shot1.csv', 'Shot2.csv', 'Shot3.csv', 'Shot4.csv', 'Shot5.csv', 'Shot6.csv', 'Shot7.csv', 'Shot8.csv', 'Shot9.csv', 'Shot10.csv', 'Shot11.csv', 'Shot12.csv', 'Shot13.csv', 'Shot14.csv', 'Shot15.csv', 'Shot16.csv', 'Shot17.csv']\n",
      "values: [243.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot0.csv\n",
      "values: [161.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot1.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  1\n",
      "we are matching:  [0. 0.]  with  [-14.19   0.  ]  particle id:  0\n",
      "values: [89.5 97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot2.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  2\n",
      "we are matching:  [-14.19   0.  ]  with  [-26.488   0.   ]  particle id:  0\n",
      "values: [37.5 97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot3.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  3\n",
      "we are matching:  [-26.488   0.   ]  with  [-35.432   0.   ]  particle id:  0\n",
      "values: [12.  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot4.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  4\n",
      "we are matching:  [-35.432   0.   ]  with  [-39.818   0.   ]  particle id:  0\n",
      "values: [11.5 97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot5.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  5\n",
      "we are matching:  [-39.818   0.   ]  with  [-39.904   0.   ]  particle id:  0\n",
      "values: [35.  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot6.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  6\n",
      "we are matching:  [-39.904   0.   ]  with  [-35.862   0.   ]  particle id:  0\n",
      "values: [79.  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot7.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  7\n",
      "we are matching:  [-35.862   0.   ]  with  [-28.294   0.   ]  particle id:  0\n",
      "values: [138.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot8.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  8\n",
      "we are matching:  [-28.294   0.   ]  with  [-18.06   0.  ]  particle id:  0\n",
      "values: [205.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot9.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  9\n",
      "we are matching:  [-18.06   0.  ]  with  [-6.536  0.   ]  particle id:  0\n",
      "values: [277.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot10.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  10\n",
      "we are matching:  [-6.536  0.   ]  with  [5.762 0.   ]  particle id:  0\n",
      "values: [345.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot11.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  11\n",
      "we are matching:  [5.762 0.   ]  with  [17.544  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [6.19132167 0.        ]\n",
      "values: [405.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot12.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  12\n",
      "we are matching:  [6.19132167 0.        ]  with  [27.778  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [9.27907242 0.        ]\n",
      "values: [450.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot13.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  13\n",
      "we are matching:  [9.27907242 0.        ]  with  [35.518  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [13.35252446  0.        ]\n",
      "values: [475.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot14.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  14\n",
      "we are matching:  [13.35252446  0.        ]  with  [39.818  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [17.21411449  0.        ]\n",
      "values: [476.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot15.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  15\n",
      "we are matching:  [17.21411449  0.        ]  with  [40.076  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [19.95735818  0.        ]\n",
      "values: [451.5  97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot16.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  16\n",
      "we are matching:  [19.95735818  0.        ]  with  [35.776  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [20.86463099  0.        ]\n",
      "values: [401.   97.5]\n",
      "read file:  C:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Synthetic_1particle_linearx\\Shot17.csv\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  17\n",
      "we are matching:  [20.86463099  0.        ]  with  [27.09  0.  ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [19.50912109  0.        ]\n",
      "NumOfDataPoints:  1\n",
      "{0: {'coords': [array([0., 0.]), array([-14.19,   0.  ]), array([-26.488,   0.   ]), array([-35.432,   0.   ]), array([-39.818,   0.   ]), array([-39.904,   0.   ]), array([-35.862,   0.   ]), array([-28.294,   0.   ]), array([-18.06,   0.  ]), array([-6.536,  0.   ]), array([5.762, 0.   ]), array([6.19132167, 0.        ]), array([9.27907242, 0.        ]), array([13.35252446,  0.        ]), array([17.21411449,  0.        ]), array([19.95735818,  0.        ]), array([20.86463099,  0.        ]), array([19.50912109,  0.        ])], 'snapshotIndexList': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 'snapshotIndexSet': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, 'learning_rate': [0.3]}}\n",
      "inline,  [[  0.           0.        ]\n",
      " [-14.19         0.        ]\n",
      " [-26.488        0.        ]\n",
      " [-35.432        0.        ]\n",
      " [-39.818        0.        ]\n",
      " [-39.904        0.        ]\n",
      " [-35.862        0.        ]\n",
      " [-28.294        0.        ]\n",
      " [-18.06         0.        ]\n",
      " [ -6.536        0.        ]\n",
      " [  5.762        0.        ]\n",
      " [  6.19132167   0.        ]\n",
      " [  9.27907242   0.        ]\n",
      " [ 13.35252446   0.        ]\n",
      " [ 17.21411449   0.        ]\n",
      " [ 19.95735818   0.        ]\n",
      " [ 20.86463099   0.        ]\n",
      " [ 19.50912109   0.        ]]\n",
      "NOS:  18\n",
      "NOS_per_Section:  18\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[  0.           0.        ]\n",
      " [-14.19         0.        ]\n",
      " [-26.488        0.        ]\n",
      " [-35.432        0.        ]\n",
      " [-39.818        0.        ]\n",
      " [-39.904        0.        ]\n",
      " [-35.862        0.        ]\n",
      " [-28.294        0.        ]\n",
      " [-18.06         0.        ]\n",
      " [ -6.536        0.        ]\n",
      " [  5.762        0.        ]\n",
      " [  6.19132167   0.        ]\n",
      " [  9.27907242   0.        ]\n",
      " [ 13.35252446   0.        ]\n",
      " [ 17.21411449   0.        ]\n",
      " [ 19.95735818   0.        ]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[  0.           0.        ]\n",
      " [-14.19         0.        ]\n",
      " [-26.488        0.        ]\n",
      " [-35.432        0.        ]\n",
      " [-39.818        0.        ]\n",
      " [-39.904        0.        ]\n",
      " [-35.862        0.        ]\n",
      " [-28.294        0.        ]\n",
      " [-18.06         0.        ]\n",
      " [ -6.536        0.        ]\n",
      " [  5.762        0.        ]\n",
      " [  6.19132167   0.        ]\n",
      " [  9.27907242   0.        ]\n",
      " [ 13.35252446   0.        ]\n",
      " [ 17.21411449   0.        ]\n",
      " [ 19.95735818   0.        ]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n",
      "NOS:  18\n",
      "NOS_per_Section:  18\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[ 71.42893249  95.23843934]\n",
      " [ 88.24512785 100.71916768]\n",
      " [ 96.36232776 107.4673252 ]\n",
      " [ 92.93190918 114.87399609]\n",
      " [ 76.03849885 121.93233398]\n",
      " [ 46.19071295 127.31525296]\n",
      " [  7.52547181 129.75036635]\n",
      " [-32.66614575 128.57989015]\n",
      " [-66.49094861 124.11685628]\n",
      " [-88.45096438 117.47287785]\n",
      " [-96.75711747 110.01261127]\n",
      " [-92.60173519 102.88840691]\n",
      " [-78.63856033  96.85158569]\n",
      " [-57.76016741  92.29931561]\n",
      " [-32.53818191  89.39803268]\n",
      " [ -5.11445811  88.20497451]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[ 71.42893249  95.23843934]\n",
      " [ 88.24512785 100.71916768]\n",
      " [ 96.36232776 107.4673252 ]\n",
      " [ 92.93190918 114.87399609]\n",
      " [ 76.03849885 121.93233398]\n",
      " [ 46.19071295 127.31525296]\n",
      " [  7.52547181 129.75036635]\n",
      " [-32.66614575 128.57989015]\n",
      " [-66.49094861 124.11685628]\n",
      " [-88.45096438 117.47287785]\n",
      " [-96.75711747 110.01261127]\n",
      " [-92.60173519 102.88840691]\n",
      " [-78.63856033  96.85158569]\n",
      " [-57.76016741  92.29931561]\n",
      " [-32.53818191  89.39803268]\n",
      " [ -5.11445811  88.20497451]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n"
     ]
    }
   ],
   "source": [
    "# main function starts here\n",
    "# define different learning rates for investigating different sorting and smoothing model\n",
    "\n",
    "alpha = np.radians(theta_degrees)  # Example rotation angle in radians\n",
    "\n",
    "rates_conditions = [learning_rate_2D, motion_randomness, learning_rate_3D, NOS_per_section]\n",
    "path_finder = pf(alpha,rates_conditions,conditions)\n",
    "\n",
    "print(\"list of files: \",os.listdir(folderName))\n",
    "sorted_filenames = sorted(os.listdir(folderName), key=lambda x: int(x.split('Shot')[1].split('.csv')[0]))\n",
    "print(sorted_filenames)\n",
    "k = 0\n",
    "for file in sorted_filenames:\n",
    "    \n",
    "    if file.endswith(\".csv\"):\n",
    "        filename = os.path.join(folderName, file)\n",
    "        input_data = pd.read_csv(filename, header=None)\n",
    "        input_data = np.array(np.transpose(input_data))\n",
    "        values =  input_data[0]\n",
    "        print(\"values:\", values)\n",
    "        print(\"read file: \", filename)\n",
    "\n",
    "        paired_values = []\n",
    "        i = 0\n",
    "        for j in range(len(values)//2):\n",
    "            \n",
    "            inputList = (values[i:i+2]- offset)*pixelResolution\n",
    "           \n",
    "            # input format, list of tuple of two elements (x,y)\n",
    "            paired_values.append(inputList)\n",
    "\n",
    "            # scambled_values = random.shuffle(paired_values.copy())\n",
    "            # print(\"paired values:\", paired_values)\n",
    "            \n",
    "            i += 2\n",
    "\n",
    "        # print(\"paired values:\", paired_values)\n",
    "        # paired_values.append(xz_proj[k])\n",
    "        path_finder.append(paired_values)\n",
    "        k += 1\n",
    "\n",
    "# Assuming path_finder.get_particle_data() returns your data as a dictionary\n",
    "sorted_particle_data = path_finder.get_particle_data()\n",
    "\n",
    "NumOfDataPoints = len(sorted_particle_data)\n",
    "print(\"NumOfDataPoints: \", NumOfDataPoints)\n",
    "\n",
    "print(sorted_particle_data)\n",
    "estimated_positions = np.zeros((NOS,3*NumOfDataPoints))\n",
    "\n",
    "\n",
    "for i in range(NumOfDataPoints):\n",
    "    print(\"inline, \",np.array(sorted_particle_data[i]['coords']))\n",
    "    estimated_positions_single = Phase4_trace_3d(conditions, np.array(sorted_particle_data[i]['coords']))\n",
    "    estimated_positions_single = smooth_points(estimated_positions_single, 'sg',NOS_per_section)\n",
    "\n",
    "    estimated_positions[:,i*3:i*3+3] = estimated_positions_single\n",
    "\n",
    "# extra synthetic data points (bypassing sorting)\n",
    "estimated_positions_single = smooth_points(Phase4_trace_3d(conditions, xz_proj), 'sg',NOS_per_section)\n",
    "\n",
    "estimated_positions_new = np.column_stack((estimated_positions, estimated_positions_single))\n",
    "\n",
    "estimated_positions_graph = np.column_stack((estimated_positions_new, real_positions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate_lists:  {0: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1.])}\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  1\n",
      "we are matching:  [0. 0.]  with  [-14.19   0.  ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  2\n",
      "we are matching:  [-14.19   0.  ]  with  [-26.488   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  3\n",
      "we are matching:  [-26.488   0.   ]  with  [-35.432   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  4\n",
      "we are matching:  [-35.432   0.   ]  with  [-39.818   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  5\n",
      "we are matching:  [-39.818   0.   ]  with  [-39.904   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  6\n",
      "we are matching:  [-39.904   0.   ]  with  [-35.862   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  7\n",
      "we are matching:  [-35.862   0.   ]  with  [-28.294   0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  8\n",
      "we are matching:  [-28.294   0.   ]  with  [-18.06   0.  ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  9\n",
      "we are matching:  [-18.06   0.  ]  with  [-6.536  0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  10\n",
      "we are matching:  [-6.536  0.   ]  with  [5.762 0.   ]  particle id:  0\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  11\n",
      "we are matching:  [5.762 0.   ]  with  [17.544  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [17.544  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  12\n",
      "we are matching:  [17.544  0.   ]  with  [27.778  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [27.778  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  13\n",
      "we are matching:  [27.778  0.   ]  with  [35.518  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [35.518  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  14\n",
      "we are matching:  [35.518  0.   ]  with  [39.818  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [39.818  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  15\n",
      "we are matching:  [39.818  0.   ]  with  [40.076  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [40.076  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  16\n",
      "we are matching:  [40.076  0.   ]  with  [35.776  0.   ]  particle id:  0\n",
      "motion randomness detected in particle:  0\n",
      "not enough data to do the reconstruction or feature disabled, use 2D historical velocity instead\n",
      "final_xy:  [35.776  0.   ]\n",
      " \n",
      "---------------------------------\n",
      "current_snapshot:  17\n",
      "we are matching:  [35.776  0.   ]  with  [27.09  0.  ]  particle id:  0\n",
      "sorted_particle_data_corrected:  {0: {'coords': [array([0., 0.]), array([-14.19,   0.  ]), array([-26.488,   0.   ]), array([-35.432,   0.   ]), array([-39.818,   0.   ]), array([-39.904,   0.   ]), array([-35.862,   0.   ]), array([-28.294,   0.   ]), array([-18.06,   0.  ]), array([-6.536,  0.   ]), array([5.762, 0.   ]), array([17.544,  0.   ]), array([27.778,  0.   ]), array([35.518,  0.   ]), array([39.818,  0.   ]), array([40.076,  0.   ]), array([35.776,  0.   ]), array([27.09,  0.  ])], 'snapshotIndexList': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 'snapshotIndexSet': {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, 'learning_rate': [0.3]}}\n",
      "sorted_particle_data_corrected[i]['coords'] [array([0., 0.]), array([-14.19,   0.  ]), array([-26.488,   0.   ]), array([-35.432,   0.   ]), array([-39.818,   0.   ]), array([-39.904,   0.   ]), array([-35.862,   0.   ]), array([-28.294,   0.   ]), array([-18.06,   0.  ]), array([-6.536,  0.   ]), array([5.762, 0.   ]), array([17.544,  0.   ]), array([27.778,  0.   ]), array([35.518,  0.   ]), array([39.818,  0.   ]), array([40.076,  0.   ]), array([35.776,  0.   ]), array([27.09,  0.  ])]\n",
      "NOS:  18\n",
      "NOS_per_Section:  18\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[  0.      0.   ]\n",
      " [-14.19    0.   ]\n",
      " [-26.488   0.   ]\n",
      " [-35.432   0.   ]\n",
      " [-39.818   0.   ]\n",
      " [-39.904   0.   ]\n",
      " [-35.862   0.   ]\n",
      " [-28.294   0.   ]\n",
      " [-18.06    0.   ]\n",
      " [ -6.536   0.   ]\n",
      " [  5.762   0.   ]\n",
      " [ 17.544   0.   ]\n",
      " [ 27.778   0.   ]\n",
      " [ 35.518   0.   ]\n",
      " [ 39.818   0.   ]\n",
      " [ 40.076   0.   ]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n",
      "proj_used_index:  0\n",
      "16\n",
      "[[  0.      0.   ]\n",
      " [-14.19    0.   ]\n",
      " [-26.488   0.   ]\n",
      " [-35.432   0.   ]\n",
      " [-39.818   0.   ]\n",
      " [-39.904   0.   ]\n",
      " [-35.862   0.   ]\n",
      " [-28.294   0.   ]\n",
      " [-18.06    0.   ]\n",
      " [ -6.536   0.   ]\n",
      " [  5.762   0.   ]\n",
      " [ 17.544   0.   ]\n",
      " [ 27.778   0.   ]\n",
      " [ 35.518   0.   ]\n",
      " [ 39.818   0.   ]\n",
      " [ 40.076   0.   ]]\n",
      "row_number_A:  62\n",
      "col_number_A:  33\n",
      "estimated_positions_corrected_graph:  [[ 4.47509583e-04 -3.08893970e+00 -8.91573083e-16]\n",
      " [ 5.96177484e-02 -3.08169059e+00 -5.30761742e-16]\n",
      " [ 1.18703890e-01 -3.07408048e+00 -2.21797202e-16]\n",
      " [ 1.77705935e-01 -3.06610938e+00  3.53205357e-17]\n",
      " [ 2.36623883e-01 -3.05777729e+00  2.40591471e-16]\n",
      " [ 2.95457734e-01 -3.04908420e+00  3.94015605e-16]\n",
      " [ 3.54207488e-01 -3.04003012e+00  4.95592936e-16]\n",
      " [ 4.12873145e-01 -3.03061504e+00  5.45323466e-16]\n",
      " [ 4.71454705e-01 -3.02083897e+00  5.43207193e-16]\n",
      " [ 5.29952168e-01 -3.01070191e+00  4.89244119e-16]\n",
      " [ 5.88365534e-01 -3.00020385e+00  3.83434242e-16]\n",
      " [ 6.46694804e-01 -2.98934480e+00  2.25777563e-16]\n",
      " [ 7.04939976e-01 -2.97812476e+00  1.62740823e-17]\n",
      " [ 7.63101051e-01 -2.96654372e+00 -2.45076201e-16]\n",
      " [ 8.21178030e-01 -2.95460169e+00 -5.58273285e-16]\n",
      " [ 8.79170911e-01 -2.94229866e+00 -9.23317172e-16]\n",
      " [ 9.37079696e-01 -2.92963464e+00 -1.34020786e-15]\n",
      " [ 9.94904383e-01 -2.91660963e+00 -1.80894535e-15]]\n"
     ]
    }
   ],
   "source": [
    "# %% recalculate learning rates\n",
    "learning_rate_floor = path_finder.get_default_learning_rate()\n",
    "learning_rate_floor = 1\n",
    "learning_rate_ceiling = 1\n",
    "\n",
    "# empirical, adjust here\n",
    "min_distance_to_origin = 0.6\n",
    "max_distance_to_origin = 6\n",
    "\n",
    "learning_rate_lists = {}\n",
    "\n",
    "for i in range(path_finder.get_total_num_of_particles()):\n",
    "    estimated_positions_individual = estimated_positions_new[:,i*3:i*3+3]\n",
    "\n",
    "    learning_rate_lists[i] = map_range(np.linalg.norm(estimated_positions_individual, axis=1), min_distance_to_origin, max_distance_to_origin, learning_rate_floor, learning_rate_ceiling)\n",
    "\n",
    "print(\"learning_rate_lists: \",learning_rate_lists)\n",
    "\n",
    "corrected_path_finder = pf(alpha,rates_conditions,conditions)\n",
    "\n",
    "corrected_path_finder.correct_learning_rate(learning_rate_lists)\n",
    "\n",
    "shotData = path_finder.get_original_shotData()\n",
    "for i in range(len(shotData)):\n",
    "        # print(\"shotData\",self.shotData[i])\n",
    "        corrected_path_finder.append(shotData[i])\n",
    "# corrected_path_finder.re_run(path_finder.get_original_shotData())\n",
    "\n",
    "sorted_particle_data_corrected = corrected_path_finder.get_particle_data()\n",
    "print(\"sorted_particle_data_corrected: \",sorted_particle_data_corrected)\n",
    "\n",
    "estimated_positions_corrected = np.zeros((NOS,3*NumOfDataPoints))\n",
    "for i in range(NumOfDataPoints):\n",
    "    print(\"sorted_particle_data_corrected[i]['coords']\",sorted_particle_data_corrected[i]['coords'])\n",
    "    estimated_positions_single = Phase4_trace_3d(conditions, np.array(sorted_particle_data_corrected[i]['coords']))\n",
    "    estimated_positions_single = smooth_points(estimated_positions_single, 'sg',NOS_per_section)\n",
    "\n",
    "    estimated_positions_corrected[:,i*3:i*3+3] = estimated_positions_single\n",
    "\n",
    "\n",
    "estimated_positions_corrected_new = estimated_positions_corrected\n",
    "\n",
    "\n",
    "# estimated_positions_corrected_graph = np.column_stack((estimated_positions_corrected_new, real_positions))\n",
    "estimated_positions_corrected_graph= estimated_positions_corrected_new\n",
    "\n",
    "print(\"estimated_positions_corrected_graph: \",estimated_positions_corrected_graph)\n",
    "\n",
    "NumOfDataPoints += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumOfDataPoints:  1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\10032\\Documents\\GitHub\\Fluid-tracking\\Phase7_Python\\XPV.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/10032/Documents/GitHub/Fluid-tracking/Phase7_Python/XPV.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mmax\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m0\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mmin\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m0\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m1e-3\u001b[39m: \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/10032/Documents/GitHub/Fluid-tracking/Phase7_Python/XPV.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ax2\u001b[39m.\u001b[39mset_xlim3d(\u001b[39mmin\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m0\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mmax\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m0\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/10032/Documents/GitHub/Fluid-tracking/Phase7_Python/XPV.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mmax\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m1\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mmin\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m1\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m1e-3\u001b[39m:   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/10032/Documents/GitHub/Fluid-tracking/Phase7_Python/XPV.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     ax2\u001b[39m.\u001b[39mset_ylim3d(\u001b[39mmin\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m1\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mmax\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m1\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/10032/Documents/GitHub/Fluid-tracking/Phase7_Python/XPV.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mmax\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m-\u001b[39m \u001b[39mmin\u001b[39m(estimated_positions_corrected_graph[:,\u001b[39m2\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]) \u001b[39m<\u001b[39m \u001b[39m1e-3\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "# Create the figure and axes\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in range(NumOfDataPoints):\n",
    "    plotting_single(estimated_positions_graph[:,i*3:i*3+3],i,ax)\n",
    "\n",
    "NumOfDataPoints -=2\n",
    "print(\"NumOfDataPoints: \",NumOfDataPoints)\n",
    "\n",
    "fig2 = plt.figure()\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "\n",
    "if max(estimated_positions_corrected_graph[:,0*3]) - min(estimated_positions_corrected_graph[:,0*3]) < 1e-3: \n",
    "    ax2.set_xlim3d(min(estimated_positions_corrected_graph[:,0*3])-1, max(estimated_positions_corrected_graph[:,0*3])+1)\n",
    "\n",
    "if max(estimated_positions_corrected_graph[:,1*3]) - min(estimated_positions_corrected_graph[:,1*3]) < 1e-3:   \n",
    "    ax2.set_ylim3d(min(estimated_positions_corrected_graph[:,1*3])-1, max(estimated_positions_corrected_graph[:,1*3])+1)\n",
    "\n",
    "if max(estimated_positions_corrected_graph[:,2*3]) - min(estimated_positions_corrected_graph[:,2*3]) < 1e-3:\n",
    "    ax2.set_zlim3d(min(estimated_positions_corrected_graph[:,2*3])-1, max(estimated_positions_corrected_graph[:,2*3])+1)\n",
    "\n",
    "x_min = ymin = z_min = float('inf')\n",
    "x_max = y_max = z_max = float('inf')\n",
    "for i in range(NumOfDataPoints):\n",
    "    plotting_single(estimated_positions_corrected_graph[:,i*3:i*3+3],i,ax2)\n",
    "    x_min = min(min(estimated_positions_corrected_graph[:,i*3]), x_min)\n",
    "    x_max = max(max(estimated_positions_corrected_graph[:,i*3]), x_max)\n",
    "    y_min = min(min(estimated_positions_corrected_graph[:,i*3+1]), y_min)\n",
    "    y_max = max(max(estimated_positions_corrected_graph[:,i*3+1]), y_max)\n",
    "    z_min = min(min(estimated_positions_corrected_graph[:,i*3+2]), z_min)\n",
    "    z_max = max(max(estimated_positions_corrected_graph[:,i*3+2]), z_max)\n",
    "\n",
    "    # print(\"estimated_positions_corrected_graph[:,i*3:i*3+3]\",estimated_positions_corrected_graph[:,i*3:i*3+3])\n",
    "# plotting_single(estimated_positions_corrected_graph,i,ax2)\n",
    "\n",
    "if x_max - x_min < 1e-3: \n",
    "    ax2.set_xlim3d(x_min-1, x_max+1)\n",
    "\n",
    "if y_max - y_min < 1e-3:\n",
    "    ax2.set_ylim3d(y_min-1, y_max+1)\n",
    "\n",
    "if z_max - z_min < 1e-3:\n",
    "    ax2.set_zlim3d(z_min-1, z_max+1)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
